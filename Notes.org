# CROCO and CROCO-WRF notes file 2021+ for DECRA
* General links and compilations
** CROCO documentation and website links:
https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.01.download.html
https://www.croco-ocean.org/
https://gitlab.inria.fr/croco-ocean -> development code
https://forum.croco-ocean.org/questions/

** WRF links:
CMS WRF for Gadi: https://github.com/coecms/WRF
CMS WRF wiki: http://climate-cms.wikis.unsw.edu.au/WRF - see bottom of
page for tutorials/instructions.

** OASIS links:
CMS OASIS wiki: http://climate-cms.wikis.unsw.edu.au/Using_Oasis
COSIMA OASIS?? https://github.com/COSIMA/oasis3-mct

** XIOS links:
A reference with tags:
https://www.nemo-ocean.eu/doc/node75.html

* CROCO v1.1 tutorials and initial runs, with some WRF/OASIS
** CROCO-only run (climatological forcing, Benguela):
-------------------------------------------------

# Initial file edits:
- Edit the name in setup_new_config.sh and run it to create a clean
  directory.
- Edit cppdefs.h with required options (e.g. name, MPI, BULK_FLUX, etc.).
- Edit croco.in for time-step, length, input and output file names and
  all sort of other settings.
- Edit crocotools_param.m for input files setup (grid res, paths to
  src directories etc).
- Edit param.h for pre-compilation parameters (grid sizes, MPI sizes
  etc.)
- Edit start.m for paths.
- Edit submit_croco.sub for PBS submission parameters.

# Pre-processing:
start matlab (you can use ../matlab_load.sh)

start
make_grid:
        to make INPUT/croco_grd.nc
make_bulk:
        to make bulk forcing atmospheric forcing, could also do
        make_forcing for flux forcing, makes INPUT/croco_blk.nc)

make_bry:
        to make boundary forcing files. Can also do make_clim to have
        climatological nuding files that nudge into the interior. Note
        that for WOA13 based climatology boundary conditions you need
        to do make_forcing first in order to create wind stress inputs
        to use in the geostrophic (/Ekman?) calculation of velocity
        boundary conditions from T/S. Creates croco_bry.nc and
        croco_bry_Z.nc

make_ini:
        to make the initial conditions.

# Compilation:
Edit jobcomp if needed (shouldn't be needed).
To compile, do (creates croco executable):
./jobcomp > jobcomp.log

# To run:
qsub submit_croco.sub

output turns up in OUTPUT. The log is in OUTPUT/croco.out

** OASIS compilation with CROCO
I could not get this working with the CMS gadi oasis3-mct install
(e.g. module load openmpi/4.0.1, module use ~access/modules, module
load oasis3-mct-local/opmi.4.0.1,
/g/data/access/apps/oasis3-mct/ompi.4.0.1/). So in the end I had to
download the latest version of oasis3-mct and compile it myself. This
is located at /g/data/e14/rmh561/software/oasis3-mct/. To compile it I
got a version of the util/make_dir/make.inc from Scott Wales setup for
Gadi (see oasis3-mct_gadi_make.inc), put it in util/make_dir, did
source setup_gadi_env.sh (with intel-compiler/2019.5.281, netcdf/4.7.1
and openmpi/4.0.2 only, I'm pretty sure) and
make -f TopMakefileOasis3 BUILDROOT=$PWD

The compilation ends up in build/

Using this I could compile CROCO using the instructions at
(https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.16.coupling.simple.html). I.e.
# define OW_COUPLING
# define MRL_WCI
Then, edit all the paths in jobcomp and notably OASIS path
PRISM_ROOT_DIR
./jobcomp >& compile_coupled.log

I had a lot of trouble getting the toy model compiled...gave up.

** WRF-only tutorial:
# Compiling:
Cloned https://github.com/coecms/WRF
cd WRF/WRF
./run_compile

to compile WPS (required for real configs to prepare inputs/bcs etc.):
cd ../WPS/
./run_compile

# Running tutorial:
Follow WRF tutorial (basics tab
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/wrf.php),
all steps - but CMS has put together scripts to do
this. http://climate-cms.wikis.unsw.edu.au/How_to_run_WRF

# steps:
mkdir /scratch/e14/rmh561/wrf/test
cd /scratch/e14/rmh561/wrf/test/
WRF_ROOT=/g/data/e14/rmh561/software/WRF/
cp $WRF_ROOT/WPS/run_WPS.sh ./
<modify this script to use the right WRF_ROOT and add the storage
flag for gdata/e14>
mkdir geogrid
mkdir metgrid
cp $WRF_ROOT/WPS/geogrid/GEOGRID.TBL geogrid
cp $WRF_ROOT/WPS/namelist.wps ./
cp $WRF_ROOT/WPS/metgrid/METGRID.TBL.ARW metgrid/METGRID.TBL
qsub run_WPS.sh
# This should successfully create the input files by running ungrid.exe, metgrid.exe and geogrid.exe

cp $WRF_ROOT/WRF/run/* ./
<modify run_mpi and run_real to use the right WRF_ROOT, WRF rather
than WRFV3 and add the storage flag for gdata/e14>

qsub run_real
qsub run_mpi

Make edits in namelist.input as listed at
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/wrf.php

Successful!

For a restart run simple change the start date, end date and set
restart = .true. in namelist.input

For the SST run, follow the instructions
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/ungrib.php#SST,
with the appropriate changes in run_WPS.sh and namelist.wps. Run
metgrid again (this overwrites the previous inputs, adding just the
SST field). And then run the model again (qsub run_real, qsub
run_mpi).

** WRF-only ETP, ERA Interim (and updated compile)
git clean -xf in /g/data/e14/rmh561/software/WRF/ to recompile. 
cd WRF/
./run_compile (takes 40 mins).
cd ../WPS/
./run_compile (takes 7 mins).

To setup WPS:
mkdir /scratch/e14/rmh561/wrf/era_interim
git clone https://github.com/coecms/wps-era ./

modify namelist.wps for appropriate domain and time period. THe page
https://www2.mmm.ucar.edu/wrf/users/namelist_best_prac_wps.html#i_j_parent_start
is useful to list all the options.

make WPSDIR=/g/data/e14/rmh561/software/WRF/WPS

This should create all the required input files. Check them (it worked
first time!).

cp $WRF_ROOT/WRF/run/* ./

Modify namelist.input following
https://www2.mmm.ucar.edu/wrf/users/namelist_best_prac_wrf.html
Note: dx and dy must match the global attributes in the met_em*.nc
files. 

qsub run_real

Modify # of processors in run_mpi according to
https://forum.mmm.ucar.edu/phpBB3/viewtopic.php?t=5082,
by default, the decomposition is done automatically (see
README.namelist), but you can set it if you want in &domains section
of namelist.input. I tried with 32 but it said too many. Successful
with 16. Now going with 20.

qsub run_mpi

Successfully run a day. Now going to run a month (Nov 2005) with
reduced output a larger time step. I got a seg fault with a 180 second
time step. Reduced to 135 and it seems to work

The month run seemed to work. Although is the SST fixed at its initial
value? - LATER NOTE: I think this is because I didn't have
sst_update=1 in namelist.input

** Thoughts on implementation 10/10/2021
*** payu: Might work. Already deals with oasis. 

croco is quite simple with
minimal input and config files. However, Croco doesn't use the
standard namelist system in croco.in which contains all the input
timing (e.g. the actual dates within which the run starts and
finishes??). 

WRF is more complicated, but does use the standard namelist system.

Maybe ask CMS what they think about using payu to run croco and WRF?

** Next steps 10/10/2021:

Otherwise I have to start getting OASIS to work. However, this may not
be worth doing myself as the Toulouse guys likely already have scripts
to do the coupling, generate the weights and make the namcouple
file...

Also, still to look into:
- WRF with varying SSTs: -> probably just need to set sst_update in
  namelist.input
- WRF compile with OASIS: Follow instructions in croco tutorial - they
  have everything that is needed. 
- CROCO and WRF timing with restarts (can payu do this for me)?
- WRF-CROCO setup (note: I guess we pass the fluxes one way and the
  SST, usurf, vsurf the other?)

** Benguela coupled setup following CROCO tutorial:
*** Setup
mkdir /scratch/e14/rmh561/croco_wrf/Benguela_coupled
mkdir ~/croco/scratch/e14/rmh561/croco_wrf/Benguela_coupled
cd ~/croco/scratch/e14/rmh561/croco_wrf/Benguela_coupled
cp ~/croco/croco_tools_src/Coupling_tools/create_config ./
<edit for paths - many>
./create_config
<edit paths in run_env>
*** Compiling CROCO:
Copy jobcomp from ~/croco/Benguela_LR_cpl/
<edit paths -> note I had a problem with PRISM_ROOT, must point at
...../oasis3-mct/build>
./make_CROCO_compil
This was successful (I got croco.oa).
*** Compiling WRF:
This looks tricky. The CROCO tutorial have used a modified version of
3.7.1. The CMS version of WRF is 4.3, that I might need to modify?

**** Uncoupled WRF (as before)
Started off by re-compiling WRF as previously (this would be in
uncoupled mode). Modified ../build.env to use openmpi/4.0.2 rather
than 4.0.1. and then did "git clean -xf" in /g/data/e14/rmh561/WRF/,
cd WRF/, ./run_compile. This submits a job (takes ~40 mins), compiled
fine. I copied all the .exe files in main/, along with the
configure.wrf and the log (compile_job.o***) to 
/scratch/e14/rmh561/croco_wrf/wrf_exes/uncoupled_c2b02af69c856/

**** Coupled WRF
Again in WRF/WRF, first ./clean -a to clean the config (after copying
the exes!!). Then, cp configure.wrf.backup to configure.wrf and modify
configure.wrf to include the oasis flags following listed in step 6 of
the CROCO tutorial under "Compiling WRF". To use this I then commented
out the section in run_compile that overwrites the configure.wrf file
(I couldn't seem to get my own qsub script to work), and added it
under git. Saved this configure.wrf in
/scratch/e14/rmh561/croco_wrf/wrf_exes/coupled_3b5c894c918387/. Then
./run_compile. This worked, but only just squeaked in with 1:21 of the
1:30 walltime! .exe's copied to coupled_3b5.... 

By the way: the oasis coupling code is in frames/module_cpl_oasis3*. 

***** Notes:
Comparing the so generated WRF/configure.wrf to the version provided
from croco
(~/croco_wrf/Benguela_coupled/wrf_in/inputs.../configure.wrf.uncoupled)
seems to show most changes are architecture related (so I want mine!),
except perhaps a few new modules in WRF included in the "compile
without OMP or without high optimization")? So this is all
architecture related?

Comparing the croco uncoupled and coupled configure.wrf shows changes
only associated with OASIS. These are the ones listed in step 6 of the
CROCO tutorial under "Compiling WRF". These are made by hand in CROCO,
so I probably just need to figure out how to hack this in the CMS WRF
version? Should be pretty easy. Simply do the steps in run_compile all
by hand, edit the configure.wrf file with the required changes, and
then do the subsequent steps with this file. TODO...

***** NOW
I also found Guillaume's NOW setup, copied to the tar.gz in
/g/data/e14/rmh561/NOW. Looks like he has versions of
configure.wrf.coupled for use on raijin (most up to date in
wrf3.5.1_red_flx??). The Oasis inclusions there look very simple
(there's only a few of them, linked to OA3MCT_ROOT_DIR. Might be
easy?!?...

**** WPS
As above, cd /g/data/e14/rmh561/WRF/WPS
./run_compile

*** Pre-processing CROCO
in croco_in edited start.m and crocotools_param.m for paths (and
needed to add topo_smooth parameter). Then ran matlab and
start
make_grid
make_bulk
make_forcing
make_bry
make_ini

This all seemed to work fine, producing the required files in /scratch/.../croco_files/

*** Pre-processing WRF
** Indo-Pacific basin-wide CROCO only configuration 30/12/2021
Note: This will be forced with the COADS climatology (Ishii et
al. 2005) and the lateral boundaries are from WOA2009 climatology,
with geostrophic and Ekman velocities (over a constant 40m depth Ekman
layer). The equatorial values for eastern and western boundaries
(between +-2-degrees) are obtained by interpolation of values outside
the equatorial band across +-2-degrees.

*** Setup to first working run
Cloned ETPcroco setup.

Changed from FRC_BRY to CLIMATOLOGY in cppdefs.h

Updated other settings to reflect changes in grid size and
parallelization.

../matlab_load.sh
start
make_grid
make_forcing
make_bulk
make_ini
make_bry

Note: Failed first time because I had the minimum longitude in degrees
east rather than negative degrees west. I guess I have to use the same
for each of the two limits. 

Note 2: Set the res and limits in crocotools_param, and then use the
printed output of make_grid to set LLm and MMm in param.h

For IPBW I picked:
lonmin =  -330;   % Minimum longitude [degree east]
lonmax =  -70;   % Maximum longitude [degree east]
latmin = -34;   % Minimum latitudeF  [degree north]
latmax = 60;   % Maximum latitude  [degree north]
dl = 1/4;

I got LLm=1039 and MMm=446

So I split my grid into 24 * 10 

Once output produced do compilation:
./jobcomp > jobcomp.log

Something wrong with netcdf libraries; for some reason the include
path wasn't working, didn't include netcdf.inc. So I hard coded
NETCDFLIB and NETCDFINC. NETCDFINC with nf-config --includedir doesn't
seem to pick up the /apps/netcdf/4.7.1/include/Intel directory (just
getting include).

Got it running but it crashed immediately with a blow-up
error. Dropping the time step by a factor of 3 to 1200s seems to have
worked. This was working with FRC_BRY and not CLIMATOLOGY.

Now trying to get a run with CLIMATOLOGY and a closed northern
boundary working. 

Note: run_start_date, run_end_date and output_time_steps are only used
when USE_CALENDAR is active, in which case DT_AVG etc. overwrite NAVG
etc.

Had to do "make_clim" in matlab to get croco_clm.nc; this doesn't seem
to work due to a netcdf close problem that I can probably fix... But
for now reverting to FRC_BRY (but keeping a closed northern
boundary).

I/O is a massive performance bottleneck - it is taking ages. Too much
output + a big domain + serial I/O?

*** Parallel I/O
There are a few utilities available to setup a parallel I/O run, which
should make things run much quicker.

Activate PARALLEL_FILES in cppdefs.h and recompile

To get this setup, first cd to Compile, source set_gadi_env.sh and do
gmake partit
gmake ncjoin

copy the partit and ncjoin executables to the appropriate
locations. Then in INPUT do:
./partit 24 10 croco_grd.nc
./partit 24 10 croco_bry.nc
./partit 24 10 croco_bry_Z.nc
./partit 24 10 croco_blk.nc
./partit 24 10 croco_frc.nc
./partit 24 10 croco_ini.nc

This creates 240 files for each type above. Then run as normal.

To rejoin at the end in OUTPUT do:
ncjoin croco_avg.*.nc

Takes a while (e.g. ~10 secs per time record for croco_avg.nc, or 5
mins for 30 days). 

This is wayyyyyyy quicker. 

Previous run speed: 20-25 days / hour on 240 CPUs, 
New run speed: 600 days / hour on 240 CPUs!!!
Equivalent to 40 years / day. That's almost as fast as ACCESS-OM2-1.

*** Spinup run
Started a single year spinup run with 5-daily output.

This ran through 3/4 of the year in 25 mins or so, and then crashed
with a blow-up. 

Combining the output does take a while. Can I run this with mpirun?

** Pacific basin-wide CROCO
As for Indo-Pacific with western boundary at -260E. 

Mask is ridiculously easy to edit. Did this to remove Atlantic and fix
bays etc. while doing make_grid
* CROCO v1.2 - January 2022
Running quickly through the updated tutorial. A few initial notes:

- They now have their own version of WRF (which includes wave coupling
  and some other things) - https://github.com/wrf-croco/WRF, this is
  version 4.2.1, and I need the same version of WPS (from
  https://github.com/wrf-model/WPS.git, with git checkout tags/v4.2)
  to get it working.

- I could use create_config.bash to create the configs directory - but
  this gives me lots of extra files that I don't think I need. So lets
  just stick with what I have.
** Benguela_LR climatological

- updated jobcomp (and setup_raw_config) in base to v1.2.
- ran setup_raw_config.
- modified paths in start.m and crocotools_param.m
- Matlab; start, make_grid, make_bulk, make_forcing, make_bry,
  make_ini
- Modify param.h (MPI settings) and cppdefs.h (MPI, FRC_BRY,
  BULK_FLUX). 
- Compilation; ran into a problem with netcdf -> needed to hard code
  NETCDFLIB and NETCDFINC In jobcomp as above for Indo-Pacific run.
- Modified output paths in croco.in
- Runs fine.
** Benguela_LR interannual 
Continued from above (actually recompiled, but all the same), following...
https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.05.prepro.matlab.inter.html

*** SODA:
in matlab make_OGCM with SODA data and download works, creating
bry_*_SODA and ini_*_SODA files.

*** CFSR: make_CFSR with data download doesn't work. Need to download by hand
according to readme in Aforc_CFSR. Have registered for an NCAR
account...

*** ERA5: 
I looked at the NCI ERA5, but it didn't seem to have all the
variables that CROCO wants. E.g. the total precipitation only seems to
be present in the monthly-averaged or monthly-averaged-by-hour
products, and CROCO wants this. I shouldn't need it - as there are
other precip variables (e.g. ACCESS-OM2 is going to use mcpr and
mlspr), but that would require hacking the CROCO-tools routines.

So instead following croco download instructions: in the Aforc_ERA5
readme, involving registering and installing api from
https://cds.climate.copernicus.eu/api-how-to. Copied ERA5 scripts to
local and modified era5_crocotools_param.py. Then in that directory
do: "python ERA5_request.py". The download for 3 months took a long
time... (1 hour+).  Then did "python ERA5_convert.py" (this creates
the files without the ERA5_ecmwf_ prefix. Then, to process pre-made
interpolated croco files added Aforc_ERA5 to matlab path in start,
then in matlab do Make_ERA5. This all worked fine and produced the
expected croco_blk_ERA5* files.

*** CMEMS/mercator glorys 1/12-degree:
Just modify parameters in crocotools_param, register for account and
do make_OGCM_mercator and it seemed to work, including
download. Creates the bry_*_mercator and ini_*_mercator files. 
*** ERA5 - mercator run 2005-01 -> 2005-03 in 3 sections.
Worked ok, after I played around a bit. DON't use USE_CALENDAR (see
strip replacements in run_croco_inter.bash - it uses ntimes). Also the
ERA5 data wasn't overlapping properly for a month and so errored in
the last time step of the month (because it couldn't find the blk
data). Increasing itolap_era5 from 1 to 2 in crocotools_param.m and
rerunning make_ERA5 fixed it and I got a succesful run.


** PBW updated

Updated PBW v1.1 configs to v1.2 and adjusted (see git history).

../matlab_load.sh
start
make_grid -> edited mask again.
make_forcing
make_bulk
make_ini
make_bry

./jobcomp > jobcomp.log

In INPUT:
~/croco/PBW/partit 16 9 croco_grd.nc
~/croco/PBW/partit 16 9 croco_ini.nc
~/croco/PBW/partit 16 9 croco_bry.nc
~/croco/PBW/partit 16 9 croco_bry_Z.nc
~/croco/PBW/partit 16 9 croco_blk.nc
~/croco/PBW/partit 16 9 croco_frc.nc

But this didn't work for some reason - the new version of partit
doesn't put every field in every bry file. Why?

Trying instead with NC4PAR - did not work either. Another option is
XIOS (which I think is what they use in CROCO-WRF). This probably
didn't work because I need to use the 4.7.1p not 4.7.1 version of the
netcdf libraries.

For now continuing with serial output...

Had a problem with too fast velocities through the ITF. Edited the
mask again to close the eastern most passage completely and then reran
through all above and started again (5 years). 

** Coupling
*** Coupling - toy [FAILED]
**** OASIS compilation:
Checked out latest OASIS3-mct version (on OASIS3-MCT_5.0 branch in
/g/data/e14/rmh561/oasis3-mct/).
Copied in my make.inc from croco configs directory.
source ~/croco/setup_gadi_env.sh
make realclean -f TopMakefileOasis3 > oasis_clean.out
make -f TopMakefileOasis3 BUILDROOT=$PWD > oasis_make.out
# Note: The buildroot was essential otherwise i got a permissions error.
Seems to have worked.
**** CROCO compilation:
Activate OW_Coupling in cppdefs.h
./jobcomp >& compile_coupled.log
Successfull.
**** TOY model compilation:
cp -r croco_src/SCRIPTS/SCRIPTS_COUPLING/TOY_IN ./.
cd TOY_IN
Modify Makefile for Gadi (took a while to get this working, see copy
in this directory).
Compiled fine. 
But the toy model requires input .nc files, which I don't have... They
can be created from model output using the create_oasis_toy_files.sh
script in the SCRIPTS_COUPLING - but I don't have a ww3 run to do this
from.

I could do this from WRF -> but need the right output (e.g. TAUX is
not output from my current ETP run).

*** Coupling - CROCO-WRF with COE WRF
Note: I started doing this with BENGUELA_LR, but realised it's
probably easier to just do an ETP config, since I already have that
running for an ETP domain. So after step 6, prior to WPS
pre-processing, I redid it for ETP_cpl. See notes for "ETP_cpl" below
for modifications. 

*** 1 Architecture setup:
cp croco_src/create_config.bash ./
edit create_config.bash for BENGUELA_cpl with all-prod-cpl.
Note; a backup of this file is inside the BENGUELA_cpl folder as
.bck. 
git init, add *
Edit myenv_mypath.sh for source setup_gadi_env.sh, paths, compilers
etc.
source myenv_mypath.sh
*** 2 OASIS compilation
As above in "coupling toy"
*** 3 CROCO pre-processing:
edit crocotools_param.m in PREPRO/croco directory
matlab:
   start 
   make_grid
   make_forcing
   make_bry
   make_ini
ls /scratch/e14/rmh561/croco/BENGUELA_cpl/CROCO_FILES/
croco_bry.nc  croco_bry_Z.nc  croco_frc.nc  croco_grd.nc  croco_ini.nc
**** ETP_cpl
Modified crocotools_param.m for a few names (didn't need to do lonmin,
latmin etc.), and modified
make_grid_from_WRF.m to use the ETP_wrf run done before. Then
matlab:
   start
   make_grid_from_WRF
   make_forcing
   make_bry
   make_ini
Take LLm and MMm from make_grid printed output and use in param.h.
Note: I messed up the BENGUELA croco input - would need to
regenerate. 
*** 4 CROCO compilation
Modify jobcomp, cppdefs.h and param.h according to Benguela_LR in CROCO_IN/
Note that the jobcomp Compile directory is just local, not in rundir
link, now. 
./jobcomp &> jobcomp_coupled.log
*** 5 WRF compilation
First try with CLEX CMS based WRF from /g/data/e14/rmh561/WRF/

The below is copied from above (including executables, which I'd
copied previously to /scratch/e14/rmh561/croco_wrf/wrf_exes/) from my
earlier compilation:
**** Uncoupled WRF (as before)
Started off by re-compiling WRF as previously (this would be in
uncoupled mode). Modified ../build.env to use openmpi/4.0.2 rather
than 4.0.1. and then did "git clean -xf" in /g/data/e14/rmh561/WRF/,
cd WRF/, ./run_compile. This submits a job (takes ~40 mins), compiled
fine. I copied all the .exe files in main/, along with the
configure.wrf and the log (compile_job.o***) to 
/scratch/e14/rmh561/croco_wrf/wrf_exes/uncoupled_c2b02af69c856/

**** Coupled WRF
Again in WRF/WRF, first ./clean -a to clean the config (after copying
the exes!!). Then, cp configure.wrf.backup to configure.wrf and modify
configure.wrf to include the oasis flags following listed in step 6 of
the CROCO tutorial under "Compiling WRF". To use this I then commented
out the section in run_compile that overwrites the configure.wrf file
(I couldn't seem to get my own qsub script to work), and added it
under git. Saved this configure.wrf in
/scratch/e14/rmh561/croco_wrf/wrf_exes/coupled_3b5c894c918387/. Then
./run_compile. This worked, but only just squeaked in with 1:21 of the
1:30 walltime! .exe's copied to coupled_3b5.... 

By the way: the oasis coupling code is in frames/module_cpl_oasis3*. 
    
*** 6 WPS compilation
As above, WPS is already compiled with executables in the WPS root
directory.

*** WPS pre-processing
For a first attempt I'm going to use the COE's ERA-Interim setup. This
was cloned from /scratch/e14/rmh561/wrf/ETP_WRF to
/scratch/e14/rmh561/wrf/ETP_WRF_clean, I adjusted the time in
namelist.wps to 2005-01 -> 2005-03 and then did make
WPSDIR=/g/data/e14/rmh561/WRF/WPS to make the files. Then copied
met_em.d01* and geo_em.d01.nc to
/scratch/e14/rmh561/croco/ETP_cpl/WRF_FILES/

*** WRF/real pre-processing
In WRF_IN, modified run_real and configure.namelist.real for paths and
to copy values in the ERA-Interim wrf (although there are some
differences). 

Note: Don't set nprocX and nprocY - leave them as -1 and it'll do it
automatically.

I couldn't get this working, so instead I just copied the already
created wrfbdy_d01 and wrfinput_d01 from ETP_WRF run previously. 

Also copied the namelist.input file from this run, and modified:
in &physics: sst_update = 1 if your are coupling with an ocean model
in &domains: num_ext_model_couple_dom = X : number of domains of the
other model.

Also made CPLMASK=1 everywhere in wrfinput_d01.

Also modified create_oasis_grids_from_wrf.sh and ran (including a
chmod +x on this file and the to_wrf_stag_grid.sh) to create
masks.wrf.nc and grids.wrf.nc ins WRF_FILES.

The CROCO tutorial for coupling then proceeds as if you were using the
toy model... It's not very well organised...

So instead I just continued trying to do WRF-CROCO.
*** OASIS setup
This is all done automatically by submit_job.sh. I tried to get this
working by modifying a number of scripts for setup etc. The OASIS
stuff seemed to work ok. However, I ended up giving up running into
qsub submission issues (e.g. app.conf wasn't seeming to be
accepted). I could probably solve these, but it's not obvious...

* Lionel's PAC12 setup
** Email

croco_bry.tar contains all the CROCO netcdf files (grd, rst, bry) (to
be copied in your CROCO_FILES directory)
wrf_files.tar contains all the WRF inputs files  (to be copied in your
WRF_FILES directory).

Myfiles_home contains various ascii files (basically the “home”
directory created using the create_config. You’ll find my cppdef, the
source code, etc. You have to modify the files indicated in the
tutorial. 


You have to download our fork or WRF (https://github.com/wrf-croco/),
OASIS, and XIOS, and compile everything.


Two minor details, you have the rename or link the bry files from eg
 croco_bry_SODA342_Y2019M5.nc
To
croco_bry_SODA342_Y2019M05.nc
for all months.

You also have to create links to the wrf files, see loop_ln_wrf.bash
in the same ftp.

I also put a configure.wrf file, you have to activate the keys:
-Dkey_cpp_xios  -Dkey_cpp_oasis3 -DUSE_MYDROP

** Procedure
All his file downloaded to /g/data/e14/rmh561/PAC12_croco-wrf/
configs put under git and cleaned up (remove unneccessary files) to
make comparisons easier.

Modified myenv_mypath.sh appropriately.

*** Compilation

Overall notes: XIOS and WRF were a pain, everything else was ok. See
below for the long list of errors I ran into. To compile XIOS I had to
use updated modules 

**** Full compile steps from scratch
General: For these full instructions I have updated the
setup_gadi_env.sh environment loads to use the compilers that Chris
Bladwell gave me that work for XIOS-2.5.

***** OASIS:

From the OASIS3-MCT_3.0 branch of
https://github.com/rmholmes/oasis3-mct (That I got from Lionel), 
in /g/data/e14/rmh561/oasis3-mct_3.0/util/make_dir:

source setup_gadi_env.sh
make -f TopMakefileOasis3 BUILDROOT=$PWD

Produces the build directory with build/lib and lib

Works on https://github.com/rmholmes/oasis3-mct/commit/165942ac2fe74ecf25e92c2131764af2bc16f040

***** XIOS-2.5:

From the xios-2.5 branch of https://github.com/rmholmes/XIOS, in
/g/data/e14/rmh561/XIOS-2.5 do "qsub build_xios_pbs.sh" (this calls
setup_gadi_env.sh and uses arch files arch/arch-X64_GADI).

Works on https://github.com/rmholmes/XIOS/commit/ad892f69a5504b32ba3b0678361951007f2dd68d

Produces bin/xios_server.exe

***** CROCO

From "gadi" branch of github.com/rmholmes/TropPacCROCO-WRF, in
CROCO_IN do 

./jobcomp > jobcomp.log

Works on https://github.com/rmholmes/TropPacCROCO-WRF/commit/ef70badd306669f9a24abc2e35c71f6507afeaa8

Produces croco, ncjoin and partit in this directory.
***** WRF:
in /g/data/e14/rmh561/WRF_croco/WRF/ from the gadi branch of my fork
of wrf-croco (on this commit,
https://github.com/rmholmes/WRF/commit/494abe0b16503aaf29e5c07515c14a57b9b1e0ed)
do 

./run_compile

Ran into many netcdf problems and some intel (avx_memmove) problems,
which I fixed eventually by playing with netcdf library paths, and
reverting to the previous version of the intel compiler that I was
using (before Chris's updates). Everything worked with the latest
netcdf (4.8.0), including parallel netcdf (4.8.0p). 

But part of this might be because my run_compile script was submitting
a job that was sourcing build.env rather than setup_gadi_env.sh!!!
Fixed that and recompiled..

**** Compile notes (old) dealing with errors.

***** XIOS Compilation:
Following instructions in README_XIOS, downloaded to
/g/data/e14/rmh561/XIOS/. Made some X64_GADI fcm, path and env files
in arch/ and then did ./make_xios --arch X64_GADI

seems to be working...

Got an error where the netcdf_par.h file was not found (i.e. netcdf4
has not been compiled with parallel option available?). Trying again
with the flag --netcdf_lib netcdf4_seq -> this could be an issue later
on. This would probably be fixed by using netcdf/4.7.1p instead of
netcdf/4.7.1 (figure this out later). 

Failed because I made a mistake in .fcm file... Now trying again
adding the --use_oasis oasis3_mct flag!

After a few more tweaks, it worked...

Does it use the XIOS files in XIOS_IN or in the base directory? They
differ, but I don't know which ones are used? Also note that there is
additional steps in jobcomp to do CPP pre-processing on the files in
croco_src/XIOS/ (where another version of these files is) and copy
them to the CROCO_IN control directory. So really there are four
versions of these files (src, XIOS_IN, CROCO_IN and the base config
directory) - Need to figure this all out when you compile CROCO below
(should automatically overwrite some of the ones in the CROCO_IN
directory - which is ok. I think the actual ones used are in
XIOS_IN/).

With XIOS-2.5 (instead of trunk) and oasis3-mct-3.0 (or any version to
be honest) I'm running into a compilation error: shared_ptr is
ambiguous. Try using Lionel's XIOS arch files?

Chris Bladwell has done this and sent me his configuration
files. Admitedly they were with different compilers but hopefully I
can just modify those compilers. 

***** OASIS Compilation:
Already done above...

Had to redo with other versions. E.g. latest is Lionels
oasis3-mct-3.0, copy make.inc into util/make_dir. Then in that
directory do source setup_gadi_env.sh and 
make -f TopMakefileOasis3 BUILDROOT=$PWD

***** CROCO Compilation:
Should be straight forward. Need to include XIOS and OASIS. There are
some funny things with the XIOS input files - but they are understood
(see above).

Modified jobcomp to be consistent with my setup.

Compilation worked with a few minor changes (from CROCO_IN).

***** WRF Compilation:
Copied CMS run_compile, my build.env and my configure.wrf from CMS
WRF/WRF/ to WRF_croco/WRF. I then modified this to add XIOS and add a
few extra flags that Lionel suggested (through a careful comparison of
CMS configure.wrf and Lionel's configure.wrf, generally staying closer
to the CMS version than Lionel's). 

THen ./run_compile to do the compile (submits job). 

I ran into some problems with OASIS. This is possibly because I hadn't
compiled in uncoupled mode yet. So now compiling in uncoupled mode
first...

Ran into some problems with netcdf.inc -> this is because of the Intel
subfolder in the version of netcdf I'm using. I had to manually
replace occurances of NETCDFPATH/include and NETCDFPATH/lib with
NETCDFPATH/include/Intel and NETCDFPATH/lib/Intel everywhere i could
find. This seemed to work (made it futher).

Turning off XIOS, OASIS and USE_MYDROP gives a successful compile
(commited configure.wrf to git, saved as
configure.wrf.uncoupled.noxios and copied exes to the ../exes
folder). However, I ran into problems with any of them activated (see
below).

Some additional checks on the issues below:
- Optimization using -O3 or -O2 makes no difference.
- the options -DCHUNK=64 and -DXEON_OPTIMIZED_WSM5 seem to have no
  effect (I thought the later would be associated with USE_MYDROP, but
  it isn't).

I've also gone through and added some other changes to periphery
variables to make sure (e.g. DBUILD_RRTMG_FAST options
etc...). Recompiling again without OASIS, XIOS or USE_MYDROP. 

***** MY_DROP

Then I ran into a problem with:

mpif90 -f90=ifort -o module_mp_wsm7.o -c -O2 -ip -fp-model precise -w
-ftz -align all -fno-alias -FR -convert big_endian -xHost -fp-model
fast=2 -no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common
-xCORE-AVX512 -I../dyn_em -I../dyn_nmm
-I/g/data/e14/rmh561/WRF_croco/WRF/external/esmf_time_f90
-I/g/data/e14/rmh561/WRF_croco/WRF/main
-I/g/data/e14/rmh561/WRF_croco/WRF/external/io_netcdf
-I/g/data/e14/rmh561/WRF_croco/WRF/external/io_int
-I/g/data/e14/rmh561/WRF_croco/WRF/frame
-I/g/data/e14/rmh561/WRF_croco/WRF/share
-I/g/data/e14/rmh561/WRF_croco/WRF/phys
-I/g/data/e14/rmh561/WRF_croco/WRF/wrftladj
-I/g/data/e14/rmh561/WRF_croco/WRF/chem
-I/g/data/e14/rmh561/WRF_croco/WRF/inc -I/g/data/e14/rmh561/XIOS//inc
-I/apps/netcdf/4.7.1/include/Intel -real-size `expr 8 \* 4` -i4
module_mp_wsm7.f90

module_mp_wsm6.f90(178): error #6784: The number of actual arguments cannot be greater than the number of dummy arguments.   [WSM62D]
         CALL wsm62D(MYDROP(ims:ime,j)                             &
--------------^

Which then caused errors later on because I couldn't find
module_mp_wsm6. So I turned off -DUSE_MYDROP.

Turns out this was a bug in the CROCO WRF version. Fixed with a bug
fix in the latest master branch.

***** XIOS
I also ran into problems with XIOS:

mpif90 -o wrf.exe -O2 -ip -fp-model precise -w -ftz -align all
-fno-alias -FR -convert big_endian -xHost -fp-model fast=2
-no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common -xCORE-AVX512
-ip -xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -align all
-fno-alias -fno-common wrf.o ../main/module_wrf_top.o libwrflib.a
/g/data/e14/rmh561/WRF_croco/WRF/external/fftpack/fftpack5/libfftpack.a
/g/data/e14/rmh561/WRF_croco/WRF/external/io_grib1/libio_grib1.a
/g/data/e14/rmh561/WRF_croco/WRF/external/io_grib_share/libio_grib_share.a
/g/data/e14/rmh561/WRF_croco/WRF/external/io_int/libwrfio_int.a
-L/g/data/e14/rmh561/WRF_croco/WRF/external/esmf_time_f90 -lesmf_time
/g/data/e14/rmh561/WRF_croco/WRF/external/RSL_LITE/librsl_lite.a
/g/data/e14/rmh561/WRF_croco/WRF/frame/module_internal_header_util.o
/g/data/e14/rmh561/WRF_croco/WRF/frame/pack_utils.o
-L/g/data/e14/rmh561/WRF_croco/WRF/external/io_netcdf -lwrfio_nf
-L/g/data/e14/rmh561/XIOS/lib -lxios -L/apps/netcdf/4.7.1/lib/Intel
-lnetcdff -lnetcdf -L/apps/netcdf/4.7.1/lib/Intel -lhdf5_hl -lhdf5 -lz

/g/data/e14/rmh561/XIOS/lib/libxios.a(icaxis_attr.o): In function `cxios_set_axis_axis_ref':
icaxis_attr.cpp:(.text+0x64): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)'

***** OASIS

However, 
Now trying with just OASIS using configure.wrf.noxios. Ran into:

  mpif90 -c -real-size `expr 8 \* 4` -i4 -O0 -fno-inline -no-ip -ip
  -fp-model precise -w -ftz -align all -fno-alias -FR -convert
  big_endian -xHost -fp-model fast=2 -no-heap-arrays -no-prec-div
  -no-prec-sqrt -fno-common -xCORE-AVX512 -I../dyn_em -I../dyn_nmm
  -I/g/data/e14/rmh561/WRF_croco/WRF/external/esmf_time_f90
  -I/g/data/e14/rmh561/WRF_croco/WRF/main
  -I/g/data/e14/rmh561/WRF_croco/WRF/external/io_netcdf
  -I/g/data/e14/rmh561/WRF_croco/WRF/external/io_int
  -I/g/data/e14/rmh561/WRF_croco/WRF/frame
  -I/g/data/e14/rmh561/WRF_croco/WRF/share
  -I/g/data/e14/rmh561/WRF_croco/WRF/phys
  -I/g/data/e14/rmh561/WRF_croco/WRF/wrftladj
  -I/g/data/e14/rmh561/WRF_croco/WRF/chem
  -I/g/data/e14/rmh561/WRF_croco/WRF/inc
  -I/g/data/e14/rmh561/oasis3-mct/build/build/lib/mct
  -I/g/data/e14/rmh561/oasis3-mct/build/build/lib/psmile.MPI1
  -I/apps/netcdf/4.7.1/include/Intel module_tiles.f90 ; \ fi

module_cpl_oasis3.f90(179): error #6285: There is no matching specific subroutine for this generic subroutine call.   [OASIS_DEF_VAR]
               CALL oasis_def_var(ssnd(jw,je,jf)%nid, ssnd(jw,je,jf)%clname, id_part, (/2,1/), OASIS_Out, ishape, OASIS_Real,ierror)
--------------------^

This could be OASIS version issues. I'm using the OASIS3-MCT_5.0
branch, it looks like Lionel is using OASIS3-MCT_3.0 (which isn't
available on the gitlab).

I am trying with the OASIS3-MCT_3.1 version from the cerfacs
github. Compiled as above instructions with some slight changes to
make sure netcdf is picked up correctly in make.inc. With this version
I instead get (at the same line):

module_cpl_oasis3.f90(179): error #6634: The shape matching rules of actual arguments and dummy arguments have been violated.   [ISHAPE]
               CALL oasis_def_var(ssnd(jw,je,jf)%nid, ssnd(jw,je,jf)%clname, id_part, (/2,1/), OASIS_Out, ishape, OASIS_Real,ierror)
----------------------------------------------------------------------------------------------------------^

According to Lionel, this was a back-compatibility error in
OASIS. Using his provided oasis3-mct-3.0 version works
(hopefully....). 

*** Running the model

Made many modifications to myjob.sh, mynamelist.sh,
SCRIPTS_TOOLBOX/MACHINE/Linux/header.Linux for paths and options.

Ran loop_ln_wrf.bash and loop_ln_croco_bry.bash to appropriately link
the input files. 

Then to run do: ./submitjob.sh

This creates a directory in this folder. Now working through the many
errors that I am running into with this...

Needed the croco executable to be croco.oa not croco

Missing SODA initial conditions file. But there are some restart files
in there (npac12_spinup11_rst1.nc). I'm using one of them as an
initial condition for now (by linking in the CROCO_FILES
directory). So for this purpose I am using "RESTART_FLAG=TRUE" -> but
this threw an error (I'm not seting up the restarts properly or
something)? 

Had a few errors in some scripts that I had to fix.

wrfexe wasn't finding the netcdf shared libraries. So needed to add to
LD_LIBRARY_PATH by hand in setup_gadi_env.sh. This worked, but then I
ran into a segmentation fault. This seems to be with XIOS, potentially
in the context files processing?

Possibly steps to fix:
- Check compile steps for netcdf includes etc; but this is probably
  not the problem given there is no reference to netcdf in the error
  files.
- Check which XIOS files are used and whether they are the same as
  Lionel used. The ones used are in XIOS_IN/

I've tried using both my updated XIOS files and Lionel's ones, with
seg faults in both cases. 

Actually looks like with Lionel's files it managed to make it further,
now running into a WRF bug (in rsl.error.0000):

-------------- FATAL CALLED ---------------
29FATAL CALLED FROM FILE:  <stdin>  LINE:    1628
30module_physics_init.F: LANDUSE_INIT: open failure for LANDUSE.TBL
31-------------------------------------------

This was because my general WRF input files weren't being linked in
properly. The location of these files was being set relative to where
my WRF executables were, and this was a custom directory - it needs to
be in main. Alternatively, change the script atm_getfile.sh (top
section) to use a different directory.

On to the next error; now it is croco_ini.nc having "scrum_time"
instead of "ocean_time". This is because I'm trying to use a restart
file as an initial file (I'm pretty sure). So went back to trying to
use RESTART_FLAG=TRUE. This works for CROCO but apparently I don't
have wrf restart files? 

So basically, given what is available, I would want a CROCO restart
and a WRF initial conditions. For now, to try to get things working, I
am starting with a fake croco_ini.nc created from crocotools using
SODA from 2005. The version of SODA that is coded into crocotools now
only reaches to 2010 - so I went with 2005. 

This seems to work - the model seems to have started - it just takes a
long time to get spun-up.

Need to ask Lionel about the wrf restarts.

Errored because of time mismatch (as expected):

bry_time limits -> 12412.54... -> 12457.54...
360.0 days in every year!? cycle_length=0.

ini - ocean_time limits -> 789004800 = day 9132.
time since initialization, seconds.

rst_1.nc - ocean_time -> 1073001600 = day 12419

So I changed the time in the ini file by hand to 1073001600 (which is
the start of 2014 given that the zero time is 1980-1-1).

This worked - then ran into an incompatible HDF5 library issue - which
was because I was sourcing the wrong environment file (build.env) in
the WRF compile step. 

As part of this I also checked whether I was using the right
setup_gadi_env.sh in OASIS, XIOS and CROCO - yes, they were all
consistent. Only WRF was different.

Compiling WRF again...

Yes seems to work fine - now XIOS is running into an error: > Error
[CObjectFactory::GetObject(const StdString & id)] : In file
'/g/data/e14/rmh561/XIOS-2.5/src/object_factory_impl.hpp', line 78 ->
[ id = T_adv, U = field ] object was not found.

I think this is because DIAGNOSTICS_TS is activated in cppdefs.h but
T_adv doesn't appear in XIOS xml scripts. I've deactivated
DIAGNOSTICS_TS and compiled CROCO again - trying again. Same error...

****
Dealing with XIOS input files:
Recompiled CROCO with DIAGNOSTICS_TS active.

Comparing Lionel's original XIOS files to my ones obtained by CROCO
compile:

iodef.xml; basically the same, uses context_croco rather than
context_roms, and groups variables in the "xios" context rather than
have them all separate.

context_croco/context_roms: The same expect context_croco uses
field_def_croco and domain_def_croco rather than without the _croco
suffix, and has two lines for s_rhoS and rho_3DS.

field_def/field_def_croco: field_def_croco.xml seems the better
version to use - it's cpp processed and has T_adv (rather than
individual components.

file_def_croco.xml; A bunch of differences, but these are just where
you define what outputs you want I think.

domain_def_croco.xml/domain_def.xml: domain_def.xml has a bunch of
extra subgrid definitions, which are also included in
context_croco/context_roms. I'm going to try removing these in
context_croco.xml for now.

context_wrf.xml; the Toulouse version has a few extra floats defined
(timestep, pq0, a2, a3, a4 etc.). Field definition is automatic
apparently. domain definition is done in this file. file definition in
file_def_wrf.xml. I'm going to retain the extra floats definitions for
now (they can't hurt)?

file_def_wrf.xml; very few differences...

So list of files to include:
iodef.xml
  context_croco.xml
    field_def_croco.xml
    file_def_croco.xml
    domain_def_croco.xml

  context_wrf
    file_def_wrf.xml

Trying again with these files cleaned up (git branch
gadi_XIOSupdated). 

IT WORKSSSS!!!!!!!!!!!!!!!!!!!!!!!!!

*** Working commits for all the components and configuration:
Configuration:
https://github.com/rmholmes/TropPacCROCO-WRF/tree/gadi
https://github.com/rmholmes/TropPacCROCO-WRF/commit/acbbf853d7e08bf10bc4e6285e0033c5c920ff71

croco base config files (e.g. setup_gadi_env.sh):
https://github.com/rmholmes/croco_setup,
https://github.com/rmholmes/croco_setup/commit/53807d2d96b94528708672fe3332dd2fc8662ce6

WRF_croco:
https://github.com/rmholmes/WRF/tree/gadi, 
https://github.com/rmholmes/WRF/commit/c923dceee6f9a7773de0a5c006b347734f4baa33

XIOS-2.5:
https://github.com/rmholmes/XIOS,
https://github.com/rmholmes/XIOS/commit/ad892f69a5504b32ba3b0678361951007f2dd68d

OASIS3-MCT 3.0:
https://github.com/rmholmes/oasis3-mct,
https://github.com/rmholmes/oasis3-mct/commit/165942ac2fe74ecf25e92c2131764af2bc16f040

CROCO code:
https://github.com/rmholmes/croco,
https://github.com/rmholmes/croco/commit/6050784f191cd4fec0d352c7ad2644c334a20038

** Model performance and benchmarking:
- Pre-running phase seems to take about 15-20 mins - mostly oasis
  setup and remapping calculations?
- Once running:
  100 CROCO time-steps in about 94 seconds
  = 1 day in about 4.5 minutes
  = 1 month in about 140 minutes / 2.3 hours
  = 1 year in about 28 hours
This is with 384 total cores
216 WRF, 24 XIOS-WRF, 96 CROCO, 48 XIOS-CROCO

So assuming a month takes 2.7 hours, the cost should be:
normal: 384*2.7*2 = 2 kSU / month = 24 kSU / year
express = 3 * normal

So to run 5-year control + smoothed SST experiments is a total of
240 kSU. That's fine.

** Model output:
WRF output is 40GB total for the month - but there is lots of 1hr
variables and many copied variables (see file_def_wrf.xml).  WRF
restart is pretty negligible (500MB). All copied to the
outputs/restarts folder.

CROCO output is very large (250GB) because of lots of 3-hourly
instantaneous outputs (not even sure why we've got these). With only
5-day output it is about 20GB. It also did not get moved to the output
folder for some reason - this was because OCE_XIOS_NAME was not
properly set in mynamelist.sh. 

CROCO restart is 1.4GB.

ascii scripts all copied to home directory which is nice.

* PAC12_75 - ocean-only CROCO base simulation.
** To-do list:
**** TODO Figure out MPI_NOLAND option
**** DONE Write script to delete job and resubmit automatically when hanging
     CLOSED: [2022-11-17 Thu 10:04]
**** DONE Put together mixing experiments
     CLOSED: [2022-05-19 Thu 11:40]
**** DONE Run 2015-2018 for analysis
     CLOSED: [2022-05-13 Fri 08:01]
**** DONE Make a gdata syncing script
     CLOSED: [2022-05-13 Fri 08:01]
**** DONE Validate 2014 high-freq output
     CLOSED: [2022-04-21 Thu 13:24]
**** DONE Email Lionel asking for:
     CLOSED: [2022-03-02 Wed 12:50]
- Restarts - where are the WRF ones. Generally, where did the CROCO
  restarts come from (I have 5 sets)? How should I bypass the
  RESTART_FLAG code.
- Why did they use SODA (and why 3.4.2) rather than mercator (I have
  mercator working).
- Ask about SODA reprocessing for vert. res. I would need; new IC (so
  depends on answer to previous questions), new bry. How did they use SODA3.4.2?
- Confirm run times and performance as a check. Have they done any
  scaling, load balancing or bottlenecks analysis? Would increasing
  the vertical resolution in the ocean have a negative effect on this?
- Ask about OASIS SST smoothing - is there some information on how to
  do it somewhere? I also have Guillaume NOW files - it might be in
  there.
- Ask about ensembles - what are they doing? Do they need these?
- Ask if I can get their file_def_wrf and file_def_croco files again
  to confirm which variables they are outputing (particularly for WRF,
  which I'm not sure about).
**** DONE Clean up setup, notes for running (compilation already done) and directories
     CLOSED: [2022-03-02 Wed 12:50]
**** DONE Check ocean parameters and make notes (mixing, vert res. etc.)
     CLOSED: [2022-03-03 Thu 15:08]
***** Vertical resolution:
In PAC12 50 level it's ~5m at the surface decreasing almost linearly
to ~30m at 250m.  

In Cherian et al. MITgcm (1/20-degree) we had 325
levels with 1m at the surface.  

In the 1/20-degree ROMS used in HT15 and Whitt 2021 we had 50 levels
with about 8m in the upper 100m (check the stretching parameters). 

I think I want at least 75, possibly 100 levels. But I also need to
think about the changes to the vertical viscosity that need to be
made.

Plotting 50, 75 and 100 levels indicates I don't get that much more
going to 100 levels. With 75 levels I have 2.5-3m at surface, 5m at
50m, 8m at 100m and 15m at 200m. That's fine.

***** Mixing parameters:
K0 = 5e-3 maximum diffusivity due to shear
Ri0 = 0.7
1e-5 interior diffusivity. 
I used K0=2e-3 for HT15, but otherwise consistent.

Viscosity; in HT15 and PAC12 used 1e-4 background value (plus BL and
shear). I guess this is ok?

**** DONE Checkout output variables - e.g. many time-independent
     CLOSED: [2022-03-03 Thu 16:57]
  quantities are coming out as time dependent (e.g. Vtransform). Why
  is "operation="once"" not working?

**** DONE ERA5 forcing:
     CLOSED: [2022-03-03 Thu 16:44]
bulk forcing options: I think I should just use the v1.2 defaults that
I used for Benguela_LR?

Do I want the SW diurnal cycle analytical?

time resolution:
Everything is hourly except LSM and SST which are daily.

**** DONE Think about whether h needs to be different with different vertical resolution
     CLOSED: [2022-03-03 Thu 15:08]
I'm happy to go on without worrying about this.
**** DONE Test 1 month offline ERA5 run
     CLOSED: [2022-03-09 Wed 12:36]
**** DONE Implement online ERA5 forcing for production runs
     CLOSED: [2022-03-09 Wed 12:36]
**** DONE Fix hanging error - why? Compile with debugging? It's croco I think
     CLOSED: [2022-04-13 Wed 13:59]
Debugging doesn't seem to give me much of a hint... The error trace
is:
forrtl: error (78): process killed (SIGTERM)
Image              PC                Routine            Line        Source             
crocox             00000000015F8294  Unknown               Unknown  Unknown
libpthread-2.28.s  0000147219B25C20  Unknown               Unknown  Unknown
hmca_bcol_ucx_p2p  00001471F2A67190  Unknown               Unknown  Unknown
libhcoll.so.1.0.1  00001471FCBDD0EC  hmca_coll_ml_barr     Unknown  Unknown
mca_coll_hcoll.so  00001471FCED8CDA  mca_coll_hcoll_ba     Unknown  Unknown
libmpi.so.40.20.2  000014721AEEBA28  MPI_Barrier           Unknown  Unknown
libmpi_mpifh.so    0000147219D7C983  Unknown               Unknown  Unknown
crocox             00000000004209C0  MAIN__                    588  main_.f
crocox             000000000041ADE2  Unknown               Unknown  Unknown
libc-2.28.so       0000147219771493  __libc_start_main     Unknown  Unknown
crocox             000000000041ACEE  Unknown               Unknown  Unknown

The problem was the MPI version - 

**** DONE Fix shflux flux bug - recompile with changed croco name
     CLOSED: [2022-03-11 Fri 10:12]
**** DONE Figure out SST bugs
     CLOSED: [2022-03-11 Fri 10:10]
**** DONE Email Lisa asking about ONLINE option
     CLOSED: [2022-03-15 Tue 08:33]
**** DONE Run CROCO 10-year 2014 spinup.
     CLOSED: [2022-04-13 Wed 13:59]
**** DONE Validate 2014 10-year spinup.
     CLOSED: [2022-04-13 Wed 13:59]
** Experiment table
| Name   | Description
| exp1   | 2014 spin-up year 1
| exp2   | 2014 spin-up year 2 (initialized from end of exp1) - note change of vertical advection scheme to VADV_ADAPT_IMP at month 4. 
| exp3   | 2014 spin-up year 3
| exp4   | 2014 spin-up year 4
| exp5   | 2014 spin-up year 5
| exp6   | 2014 spin-up year 6
| exp7   | 2014 spin-up year 7
| exp8   | 2014 spin-up year 8
| exp9   | 2014 spin-up year 9
| exp10  | 2014 spin-up year 10
| exp11  | 2014-2018 Control simulation (XIOS output updated)
| exp12  | 2014+ PP81 simulation with PP81 interior shear replacing KPP interior shear
** Performance/scaling table (new):
Time step = 300 seconds
Grid size = 1600 * 502
In the first section below nXIOS was chosen large so that I/O was not a bottle neck (it may have been for the very largest below).
| NX*NY  | TOTC | XIOS | TOT  | Tile    | qu | s/100ts     |
| 8*3    | 24   | 32   | 56   | 200*167 | bw | 618.7 (/10) |
| 17*5   | 85   | 55   | 140  | 94*100  | bw | 130.4       |
| 26*8   | 208  | 72   | 280  | 61*63   | bw | 54.3        |
| 33*10  | 330  | 118  | 448  | 49*50   | bw | 37.6        |
| 39*12  | 468  | 120  | 588  | 41*42   | bw | 26.5 (*2)   |
| 43*13  | 559  | 197  | 756  | 37*39   | bw | 23.1 (*2)   |
| 48*15  | 720  | 176  | 896  | 33*33   | bw | 19.0 (*2)   |
| 52*16  | 832  | 232  | 1064 | 31*31   | bw | 16.3 (*4)   |
| 55*17  | 935  | 269  | 1204 | 29*30   | bw | 14.83 (*4)  |
| 59*19  | 1121 | 251  | 1372 | 27*26   | bw | 12.44 (*5)  |
| 71*22  | 1562 | 286  | 1848 | 23*23   | bw | 9.8   (*10) | Note: May have been some I/O bottleneck - some pauses in log
| 89*28  | 2492 | 560  | 3052 | 18*18   | bw | 7.2   (*20) | " as above.
| ------------------------------------------------------------
| 8*3    | 24   | 28   |      | 200*167 | bw | 560   (/10) | No output. 
| 17*5   | 85   | 28   |      | 94*100  | bw | 117  (/2)   |
| 26*8   | 208  | 28   |      | 61*63   | bw | 47.8        |
| 39*12  | 468  | 28   |      | 41*42   | bw | 23.0 (*3)   |
| 48*15  | 720  | 28   |      | 33*33   | bw | 16.1 (*5)   |
| 48*15  | 720  | 48   |      | 33*33   | cl | 15.8 (*5)   |
| 59*19  | 1121 | 28   |      | 27*26   | bw | 11.2 (*5)   |
| 71*22  | 1562 | 28   |      | 23*23   | bw | 7.9   (*10) | 
| 89*28  | 2492 | 28   |      | 18*18   | bw | 5.9   (*20) | 9:57 walltime (so 6.69s with initialization)
| ---------------------------------------------------------- s/31 days (wallt) | CPU-HR/year |
| 48*15  | 720  | 8    | 728  | 33*33   | bw |             | Can't seem to get this one working... always hangs...
| 48*15  | 720  | 10   | 738  | 33*33   | bw |             | or this one...
| 48*15  | 720  | 18   | 738  | 33*33   | bw |             | 1788 (20.02)    | 4316 | 
| 48*15  | 720  | 36   | 756  | 33*33   | bw | 19.0 (*2)   | 1788 (20.02)    | 4421 |
| 48*15  | 720  | 64   | 784  | 33*33   | bw |             | 1798 (20.14)    | 4610 |
| 71*22  | 1562 | 18   | 1580 | 23*23   | bw |             | "
| 71*22  | 1562 | 34   | 1596 | 23*23   | bw |             | Likewise, doesn't seem to run... Something about XIOS memory?
| 71*22  | 1562 | 62   | 1624 | 23*23   | bw |             | "
| 71*22  | 1562 | 80   | 1652 | 23*23   | bw |             | "
| 71*22  | 1562 | 108  | 1670 | 23*23   | bw |             | 962 (10.8)  / 897 -> these numbers mean nothing... | 5254 |
| 71*22  | 1562 | 136  | 1698 | 23*23   | bw |             | 914 (10.2)      | 5076 |
| 71*22  | 1562 | 164  | 1726 | 23*23   | bw |             | 907 (10.2)      | 5120 |
| 71*22  | 1562 | 164  | 1726 | 23*23   | bw |             | 922 (...)

Best configuration (with XIOS and initialization/finalization):
48*15=720 + 36 = 756. Cost of about 4500 CPU-hr/year (5600 SUs on broadwell) at 5.9 hours/year.

*** Performance/scaling table (relatively old):
Time step = 300 seconds
Grid size = 1600 * 502
| Name          | NX*NY  | XIOS | TOT | Tile   | queue    | s/100ts | hr/365d | SU/365 day | Notes | 
| PAC12 default | 12*8   | 48   | 144 | 133*62 | normal   | 
| 16*8          | 16*8   | 12   | 140 | 100*62 | normalbw | 80      | 23.36   | 4088       | 
| 32*10         | 32*10  | 16   | 336 | 50*50  | normalbw | 35.9    | 10.48   | 4401       |
| 24*9          | 24*9   | 8    | 224 | 66*55  | normalbw | 
| 48*15         | 48*15  | 36   | 756 | 33*33  | normalbw | 19.5    | 5.7     | 5386       | 15 min (3 day run time for XIOS)
| 48*15         | 48*15  | 64   | 784 | 33*33  | normalbw | 19.5    | 5.7     | 5386       | 13.6 min (3 day run time for XIOS)
| 32*10 (44)    | 32*10  | 44   | 364 | 50*50  | normalbw | 35.04   | 10.23   | 4650       | 29 min total walltime for 12 day run (12 day run, 7 mins for restart file creation).
| 32*10 E5o     | 32*10  | 44   | 364 | 50*50  | normalbw | 34      | 9.92    | 4513       | 
| 32*10 E5o NC4 | 32*10  | 44   | 364 | 50*50  | normalbw | 34      | 9.92    | 4513       | 5:23 walltime for 3 day run.
| 32*10 E5o NC4 | 32*10  | 16   | 336 | 50*50  | normalbw | 34      | 9.92    | 4513       | 
| 32*10 E5o NC4 | 32*10  | 44   | 364 | 50*50  | normalbw | 34.7    | 10.13   | 4609       | 2:32 for 3m =~ 10h/year.
| 48*15 E5o NC4 | 48*15  | 36   | 756 | 33*33  | normalbw | 19.5    | 5.7     | 5386       | 1:18 for 3m =~ 5.3h/year at cost of 5052SU/year great!
** Procedure
*** Configs setup
Edit create_config for PAC12_75 and run.
git init/commit.
edit various scripts for consistency with PAC12.

Question with cppdefs.h: It should be SFLUX_CFB - I kept this as
opposed to Lionel's SMFLUX_CFB.

Need to ask about the surface forcing for ERA5 forced run. I must use
BULK_FLUX - but are the settings correct?

*** Pre-processing:
**** Grid file and general setup:
Horizontal resolution: PAC12 is from 175.3ish to 290.6ish, -15.5ish to
20.2ish. That's big enough for me -> use their grid file. So copied
PAC12/croco_npac12_v2_InterpOnSODAgrd.nc to
PAC12_75/croco_grd.nc. This was probably created with WRF and if I
want the configurations to be consistent this is what makes the most
sense. It is in fact about 1/13-degree resolution, rather than
1/12. But it makes sense to use this.

Linked directory in scratch PAC12_75 to CROCO_FILES in
/g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75/

**** SODA 3.4.2 ini and bry processing

Made script "download_SODA342.bash" to download SODA3.4.2 data
following readme.txt from Lisa. Downloading data (about 20GB/year,
this is global).

Modified script "process_SODA342_outputs.csh" to extract monthly data
over just the region of interest and save to the same file.

Modified matlab script "add_UVbar.m" to add barotropic velocities and
then save back into new files again in same directory. Run with matlab
"start; add_UVbar;".

Also did the same for the last month of 2013 and the first month of
2019 to have some overlap.

Once done - run loop_ln_croco_bry.bash to correct file names %02.

**** ERA5 forcing

Copied ERA5_configs from Benguela_LR and modified paths etc. in
era5_crocotools_param.py. Then do "python ERA5_request.py". With
crocotools_param is downloading from 172 to 297-degrees (ok), -20 to
24 (ok). It takes about 20 mins for 1 month, so it will take about 20
hours to download it all... (set running on a detached screen on
rmh561@gadi-login-06).

Then do "python ERA5_convert.py".

Then add Aforc_ERA5 to start.m and do qsub make_ERA5.sub; which runs
matlab with "start;make_ERA5;" to make the blk_ERA5* files. This is
also very slow...

This is creating very large files (50GB/file - so 3TB of files). It
would be better to do the processing online (do offline first for 2
months, then try).

ONLINE works fine - see below. Conversion to CROCO variable types
already done with ERA5_convert.py.

**** XIOS 
in PREPRO/XIOS run ./process_xios_xml.sh

This creates and populates the XIOS_IN folder. The resulting files are
almost completely consistent with those in PAC12 with some
improvements. I will need to modify file_def_croco.xml for my output
(TBC).
**** Compile CROCO
./jobcomp > jobcomp.log 

Had to deal with some issues with XIOS being compiled with OASIS. See
changes to jobcomp and jobcomp.log. Otherwise worked fine.
*** Running, parallel settings and performance:
All the above done for 2013-12 -> 2014-02 and then a one-month test
run was attempted. ./submitjob.sh

Ran into a weird permissions error (needed chmod u+x
SCRIPTS_TOOLBOX/*.sh). 

croco executable needed to be croco.frc - renamed.

Worked through another few errors (e.g. initial file not properly
named)!

Default (PAC12) is 12*8 = 96, + 48 XIOS cores.
Grid is 1600 * 500, so a tile is 133*62. 
ACCESS-OM2-025 is 1824 processors for. 1455 ocean, for 1440*1080 with
tiling of 48*40 (so 30*27). 

Queues: normal - 48/192GB, broadwell -> 28/256GB.

So sticking with 48 XIOS cores. 32*10 = 320. XIOS cores should be
1/10-1/50th of NEMO cores (according to
https://www.nemo-ocean.eu/doc/node75.html), so if I do 16 XIOS cores I
get 7 nodes on normal (320 + 16 = 336 = 7*48). On normalbw I can do
the same (320 + 16 = 336 = 12 * 28). Let's do broadwell - it's
cheaper. So conclusion:
Try 32 * 10 = 320, +16 XIOS cores (TBC if XIOS is too low). 

Updated GADI scripts to do things properly with memory usage etc.

Is the hanging issue (just before main time stepping) a random
Gadi-related error (e.g. on certain nodes?). Or do I have to have
compiling folder and executable the same.

Trying also ERA-online interpolation of forcing files; activate ONLINE
and ERA_ECMWF in cppdefs.h, set OCE_FILES_ONLINEDIR in
myenv_mypath.sh, change interponline and frc_ext in
mynamelist.sh. This worked, after a few changes to some
scripts. Doesn't seem to slow things down at all...

Writing of restarts seems very slow... I don't think this is XIOS! ->
it only deals with output, which seems fine. It didn't seem like it
was writing the restart in parallel - but it might be writing the
output files in parallel? Trying instead with NC4PAR - Yes this fixed
it easily.

*** First spin-up run: Problems
Ran into problems:

**** Ran into an annoying problem with the oce_getbdy.sh script that,
   for multiple-month runs, concatenates the bry files of multiple
   months into a single file. This wasn't behaving as expected because
   it expected monthly-averaged boundary forcing files and I had 5-day
   averages. Fixed by hard-coding the time ranges into this file (it
   should throw an error if this at any time doesn't work).

**** Problem with ONLINE option: There is some problem - SST seems way
   too high. Outputing shflx's to see why - but there is a problem
   with the total shflux vs. shflx - I need to figure this out. Easy
   solution would be to change the CROCO code variable name to shflx
   from shflux.

The problem is with the ONLINE option. With this option the wind
stress is way to weak - some units issue? Comparing croco_blk and the
raw files they seem to match - so it's somewhere in the code. 

BULK_LW - should this be on or not?

Started blk_ERA5 processing as backup... Should do for 2014 at least
so I can get the spin-up going....

Looks like without online forcing you can't run jobs for more than a
month...

Turns out that the ONLINE option bug was a bug, and has been fixed
with croco v1.2.1. This has been downloaded and I'm reverting back to
ONLINE version (much easier than running only a month at a time with
the hanging bug).

**** Bulk fluxes:
Also, I did not have the current feedback activated. I have now
activated SFLUX_CFB in cppdefs.h.

On the bulk fluxes; in OA_COUPLED mode BULK_FLUX is deactivated -
i.e. all the fluxes are determined by WRF.

**** XIOS issues; why do I need different variable names in different
files/output frequencies? That's frustrating...

**** Hanging issue

Seems pretty regular, and similar on normal and normalbw. I received
the following from Paola:

It's possible that is a MPI error, Random MPI errors are very
difficult  debug.  Also seems you are running an IO server
(xios_server.exe) which complicates debugging a great deal.

A few things to try:
- Different MPI compilers
- Switching to normal.

If not, contact NCI to examine a hung job. They have more access. In
particular Ben Menadue.

Changing to openmpi/4.0.1 instead of 4.0.2 for XIOS and CROCO seems to
have done the trick and fixed this. Spoke too soon, I don't think it
has - actually, maybe this is linked to XIOS alone (see below in
Control run for next steps).

*** First spin-up: Procedure

Run successfully with ONLINE and CFB activated for a year in 3 month
chunks (about 2.5 hours per chunk with 32*10). 

Setup for second year: 

mkdir rundir/PAC12_75_exp2_restarts
mkdir rundir/PAC12_75_exp2_restarts/20131231
cp rundir/PAC12_75_exp1_restarts/20141231/croco_rst_20141231.nc rundir/PAC12_75_exp2_restarts/20131231/croco_rst_20131231.nc
ncap2 -A -s scrum_time=1073001600 rundir/PAC12_75_exp2_restarts/20131231/croco_rst_20131231.nc rundir/PAC12_75_exp2_restarts/20131231/croco_rst_20131231.nc

Also added u_sq and v_sq diagnostics for KE calculation during
spin-up.

Still couldn't figure out XIOS multiple file same variable problem -
something wrong with my xml file format?

For third year I will run with 48*15 (36 XIOS) as its faster and not
much more expensive. I've set this up for a year.

It errored at the end of the year because it couldn't find forcing for
2015M01 (T2M) - which is weird since the file was there and linked
in. Reverted to 3 month block runs.

Lionel suggests going to VADV_ADAPT_IMP because it gets around some
Courant number issues in the vertical advection and should be more
stable (so I can increase the time step??). I've started this for the
2nd 3 month block of the 2nd spin-up year.

Changing to openmpi/4.0.1 instead of 4.0.2 for XIOS and CROCO
compilation may have solved the hanging issue. I'm 2 out of 2 so far
new runs.

Successfully run 10 years of this spinup (exp01-exp10), with the only
inconsistency being the change to VADV_ADAPT_IMP in the second year.

*** First spin-up: Validation

See 2014_Validation. Looks ok. I'm a bit worried about the deep
EUC. This is also deeper than ACCESS-OM2-025 and ACCESS-OM2-01. Why?
Too much mixing higher in the water column.

The EUC magnitude looks consistent with other model solutions (SODA
ACCESS-OM2) but about 50m deeper than in TAO.

The NECC is much better than ACCESS-OM2. In fact, the profile of SSH
is much better in CROCO than in ACCESS-OM2 - I guess because the mean
is subtracted off this really is because of the significant lack of an
NECC trough.

*** 2014-2019 Control run:

Testing: See "Notes on XIOS diagnostics below" and the
PAC12_75_XIOStest branch commit messages.

Setup for a 1 year run in 2014 with more diagnostics.

10/5/2022: Analysis seems ok. Continuing for another 4 years to end
of 2018 (with slightly reduced diagnostics).

** Notes on diagnostics (XIOS and hanging issue):

Update XIOS output: Trying a one-month test run with increased XIOS
output (multiple different files), initialized for 2014 from the end
of exp10.

I run into the hanging error again if I try to play with the XIOS .xml
files. There is some issue here. Need to nail down exactly what it
is.

It appears to be with the 1-day average (or 3-day average) block in
file_def_croco.xml. When this is commented out it works. Could the
problem be to do with the @exp_@... thing? Trying without that (its
accounted for in the folder names anyway!).  Yes this seems to work -
but then runs into the same variable name issue again, and this is
even with that file disabled!!! What the hell is going on here?

For some reason everything keeps going into the same file - that's why
the name issue comes up. Why can't I separate out the files? There's
some inheritance issue going on.

The issue is with copying the field_group id. I guess id's have to be
separate for everything. This fixes it. In fact, you don't even need
to give id's -> I guess these are only for referring back to the same
group?

However, I still get a hanging issue if I try to do this with the full
file...

So I guess I have to build things up gradually to figure out what the
problems are....

When using operation="once" files you still need to provide a
output_freq (just pick 1h).

DIAGNOSTICS_TS_ADV allows you to switch between flux-form and
divergence form for advection terms.

What is DIAGNOSTICS_TSVAR??

T_adv/S_adv didn't exist - needed to make in field_def...

The variable names in XIOS .xml files are case sensitive.

The variables are all defined in croco_src/XIOS/send_xios_diags.F They
don't all exist (e.g. the components of advection).

I think I've got something that works now...

** PP81 mixing experiment

Added code in PP81. Setup to simply swap this for the interior KPP
scheme. Seems to work pretty well. Definitely has cleaner
fields. Changes are modest but significant (what about compared to
biases??).

* PAC12_75_CPL - coupled CROCO-WRF simulation
CROCO-WRF 1/12-degree simulation initialized from PAC12_75 CROCO-only
run
** To-do list:
**** TODO Figure out hanging error and fix it!
**** TODO Come up with a better way to keep track of executables with hashes (when I start doing more mixing runs)
**** TODO Some issue with AVX in the broadwell nodes - I can't use them?
**** TODO Figure out what SST is used in the Atlantic.
**** DONE Rerun noTWB and noMesoscaleCFB simulations with masked smoothing.
     CLOSED: [2022-11-30 Wed 15:58]
**** DONE Figure out SST-smoothing remapping
     CLOSED: [2022-05-23 Mon 12:41]
**** DONE Run a no current feedback simulation?
     CLOSED: [2022-06-27 Mon 17:41]
**** DONE LOAD BALANCING - optimize
     CLOSED: [2022-06-27 Mon 17:41]
**** DONE Validate control 2014
     CLOSED: [2022-05-20 Fri 12:01]
**** DONE Clean-up smoothing script to use no depth in y.
     CLOSED: [2022-11-21 Mon 13:48]
**** DONE Tweak smoothing script to only smooth over a specified region
     CLOSED: [2022-11-21 Mon 13:48]
**** DONE Fix DIAGNOSTICS_EDDY and include eddy terms by default.
     CLOSED: [2022-06-29 Wed 17:10]
**** DONE Write script to delete job and resubmit automatically when hanging
     CLOSED: [2022-06-29 Wed 17:10]
Note: On a non-WRF-restart run it will take 20 mins to create the
OASIS remapping files. This will occur when croco.log is at the line:
      GET_INITIAL: Restarted from day =  12419.0000 rec=   1(  840961,  20,****).
and there are no output files (either wrf or croco).

Something like doing the MPI_LAUNCH step in the job script as a
background job (with &) and then sleeping a specified time then
checking might work?

I think this is what I want: https://unix.stackexchange.com/questions/637002/how-do-i-foreground-a-job-in-a-script

Basically: 
Have some option that activates the following check set:
- put main job in background with known PID
- Wait a specified time
- Check whether "MAIN: time stepping..." exists:
  - if not, wait some more.
  - if yes, then check if the first title line of stats exists.
    - if not: Kill PID
    - if yes: Put main job back in foreground.

**** DONE Think about WRF output
     CLOSED: [2022-05-13 Fri 08:01]
**** DONE Run control 2015-2018
     CLOSED: [2022-05-13 Fri 08:00]
**** DONE Write gdata syncing script
     CLOSED: [2022-05-13 Fri 08:01]
**** DONE Update scripts to PAC12_75 (e.g. MACHINE options)
     CLOSED: [2022-04-20 Wed 15:19]
**** DONE Run control 2014
     CLOSED: [2022-04-21 Thu 13:24]
** Experiment table
Note on branches: All general updates should be done on PAC12_75_cpl
branch (kept up to date), this is effectively "main".
| Name   | Description
| exp01  | 2014 Control year 1, started from exp10end      | DONE Output linked into exp02.
| exp02  | 2015-2018 Control run f                         | DONE
| exp03  | 2014+ smooth SST run, started from exp10end     | DELETED
| exp04  | exp03 E2: initialized from exp8end              | DELETED
| exp05  | PP81 mixing, started from exp10end. TIW-output  | DONE
| exp06  | NoCFB, started from exp10end.                   | DELETED
| exp07  | NoMesoCFB, started from exp10end.               | DELETED
| exp08  | Control E2, initialized from exp8end.           | DONE
| exp09  | Control E3, initialized from exp9end. TIW-output| DONE
| exp10  | Control E4, initialized from exp7end.           | DONE
| exp11  | Control E5, initialized from exp6end.           | DONE
| exp12  | As for exp03 but with tanh-masked smoothing     | DONE
| exp13  | As for exp07 but with tanh-masked smoothing     | DONE
| exp14  | exp12 E2, initialized from exp8end.             | DONE
| exp15  | exp13 E2, initialized from exp8end.             | DONE
| exp16  | exp06 E2, initialized from exp8end.             | DELETED
| exp17  | exp12 E3, initialized from exp9end. TIW-output  | DONE
| exp18  | exp13 E3, initialized from exp9end. TIW-output  | DONE
| exp19  | exp06 E3, initialized from exp9end. TIW-output  | DELETED
| exp20  | exp12 E4, initialized from exp7end.             | DONE
| exp21  | exp13 E4, initialized from exp7end.             | DONE
| exp22  | exp06 E4, initialized from exp7end.             | DELETED
| exp23  | exp12 E5, initialized from exp6end. TIW-output  | DONE
| exp24  | exp13 E5, initialized from exp6end.             | DONE
| exp25  | exp06 E5, initialized from exp6end.             | DELETED
| exp26  | NoCFB-TIW. from exp10end            TIW-outputW | DONE
| exp27  | NoCFB-TIW. from exp8end                         | DONE
| exp28  | NoCFB-TIW. from exp9end                         | DONE
| exp29  | NoCFB-TIW. from exp7end                         | DONE
| exp30  | NoCFB-TIW. from exp6end                         | DONE
| exp09rr| re-run 2015+ with TIW-outputF                   | -3 RUNNING
| exp17rr| re-run 2015+ with TIW-outputF                   | -0 RUNNING
| exp18rr| re-run 2015+ with TIW-outputF                   | -4 RUNNING
| exp26rr| re-run 2015+ with TIW-outputF                   | -2 RUNNING
TIW-outputW has updated output to include the vertical velocity and
omega.

TIW-outputF will be updated to include full-depth output (needed for
heat budget). To do this I want to rerun:
exp09 2015+: Control
exp17 2015+: NoTFB
exp18 2015+: NoMesoCFB
exp26 2015+: NoCFB

Possible deletions (22/3/2023):
Keeping hourly WRF data from 2014 just in case I want to run an
ocean-only (could probably do this for only one ensemble).

** Performance/scaling:
OCN time step = 300 seconds
ATM time step = 30 seconds
Coupling frequency = 3600 seconds
OCN grid = 1600 * 502
ATM grid = 522 * 168
| Name      | OCN/XIOS/TOT | OCN T  | ATM/XIOS/TOT | ATM T | TOT | queue    | s/100ts | hr/365d | SU/365d | 1m full job timing
| PAC12 def | 12*8/48/144  | 133*62 | 18*12/24/240 | 29*14 | 384 | normal   | 105     | 30.7    | 23600   | 2:58 =~ 35.6h/year at 27kSU/year
| PAC12 n2  | 20*8/32/192  | 80*62  | 29*12/36/384 | 18*14 | 576 | normal   | 62.5    | 18.25   | 21000   | 1:38 =~ 19.6h/year at 23kSU/year
| PAC12 n3  | 20*8/32/192  | 80*62  | 29*14/26/432 | 18*12 | 624 | normal   | 62.05   | 18.12   | 22600   | 1:36 =~ 19.2h/year at 24kSU/year
| PAC12 n4  | 32*10/16/336 | 50*50  | 29*14/26/432 | 18*12 | 768 | normal   | 53.1    | 15.5    | 23800   | 1:23 =~ 16.6h/year at 25.5kSU/year
NOTE: The PAC12 n4 seems to pause every hour. I only have ATM output every hour. Not enough ATM XIOS processors?
| PAC12 n5  | 25*8/24/224  | 64*62  | 29*14/42/448 | 18*12 | 672 | normal   | 55      | 16.06   | 21500   | 1:27.5 =~ 17.5h/year at 23.5kSU/year
This one hung once...

The second number in the option NLOGPRT in namcouple allows me to
output time statistics. Try it! 

Can also use the Lucia tool by setting this to -1. This seems like
it'd be the best way to check load-balancing. Check util/lucia/README
in oasis3-mct directory.

To run: in /g/data/e14/rmh561/croco/src/oasis3-mct/util/lucia do:
source ~/croco/setup_gadi_env.sh
lucia -c
Produces lucia.exe

** Performance/scaling updated:
OCN time step = 300 seconds
ATM time step = 30 seconds
Coupling frequency = 3600 seconds
OCN grid = 1600 * 502
ATM grid = 522 * 168
All below on 2-day run on cascade lake (normal/express):
| ATM       | ATM X |  OCN     | OCN X | s/100ts | hr/365d | CPU-hr/yr | LUC A | LUC O | LUC A W | LUC O W | CPU-hr/yr (LUC A) |
| 29*14=406 | 42    | 25*8=200 | 24    | 52.2    | 15.2    |           | 286.0 | 271.3 | 0.02    | 14.64   | 9743
| 29*14=406 | 42    | 25*8=200 | 24    | 52.8    | 15.4    |           | 286.9 | 272.1 | 0.01    | 14.76   | 9773
| 32*12=384 | 48    | 25*8=200 | 40    |         |         |           | 355.4 | 269.2 | 0.02    | 86.2    | 12107
| 29*14=406 | 58    | 26*8=208 | 48    | 54.0    |         |           | 287.8 | 267.3 | 0.01    | 20.5    | 10504
| 29*14=406 | 34    | 26*8=208 | 24    |         |         |           | 288.5 | 263.3 | 0.01    | 25.08   | 9828
| 29*14=406 | 34    | 24*7=168 | 16    | 59.0    |         |           | 288.7 | 322.5 | 33.84   | 0.02    | 10201
| 29*14=406 | 66    | 25*8=200 | 48    |         |         |           | 288.3 | 269.2 | 0.01    | 19.1    | 10522
| 29*14=406 | 38    | 25*8=200 | 30    |         |         |           | 290.0 | 273.4 | 0.01    | 16.52   | 9908
| 29*14=406 | 46    | 25*8=200 | 20    |         |         |           | 287.9 | 269.5 | 0.01    | 18.31   | 9807

Best configuration :
29*14=406 + 42 + 25*8=200 + 24 = 672. Cost of about 11000 CPU-hr/year (22kSU on normal) at 16.4 hours/year

83/31+3*81/30+3*84/31 = 16.43 hours/year
Cost = 11040 CPU-hr/year

** Model details: Data products, parameterizations etc.

We are using SODA3.4.2 (Carton et al. 2018 J Climate), which is
effecively 1/4-degree MOM5 (ocean sea-ice version of CM2.5) forced
with ERA Interim and assimilation T-S profiles and satellite
SSH/SST. We use 5-daily output. It may not be ideal as the SEC seems
particularly bad (i.e. basically eastward surface velocities even
though we are not dominated by El Nino?).

ERA-5 is used for surface forcing and atmospheric BCs (Hersbach
2022). It has a horizontal resolution of 31km (basically 1/4-degree)
and data at hourly frequency. Uses 4D-var.

I think the atmospheric BCs are only at 6-hourly resolution? But in
forced mode it uses hourly data.

Parameterizations, physics, numerics: 
CROCO: 
- Open BCs W,N,S using boundary forcing (no nudging layer). No river
  inputs, no tides.
- 3rd-order upwind horizontal momentum with no explicit horizontal viscosity.
- 3rd-order upwind rotated horizontal tracer advection with no
  explicit horizontal diffusivity.
- Semi-implicit vertical tracer and momentum advection using spplines
  (Shchepetkin 2015 OM).
- KPP vertical mixing (no double diffusion or Langmuir, both BBL and
  SML, convection and non-local).
Ocean-only version:
- Update bulk fluxes (COARE3p0). 
- Current feedback -> CFB_STRESS and CFB_Wind_TRA. These are the
wind-stress correction method based on the stress-correction (section
3.4.2 of Renault et al. 2020). The second is a tracer flux correction
(i.e. heat flux) just to correction the wind speed used in the heat
flux calculation?

WRF (see WRF_src: WRF/run/README.namelist).
- micro-physics: WSM 6-class graupal. 
- Long-wave physics: Goddard longwave scheme (Chou et al. 2001, https://esrl.noaa.gov/gsd/wrfportal/namelist_input_options.html))
- Short-wave physics: Goddard shortwave scheme
- Surface layer: M-O similarity
- Land-surface model: Noah land-surface model.
- Boundary layer physics: YSU scheme.
- Cumulus/convection physics: Multi-scale Kain-Fritsch scheme (Zheng
et al. 2016 MWR, https://esrl.noaa.gov/gsd/wrfportal/namelist_input_options.html).
- No shallow cumulus.

extra note: We are also using "MY_DROP" - which uses a climatological
droplet size rather than a constant size (Lionel's explanation). 

Some extra notes:
Lisa sent me some validatin they did for a few runs comparing cu11 (as
above) and cu14. 

** Procedure
*** Initial setup
Cloned PAC12 original setup into PAC12_75_cpl in ~/croco

Setup directories:
mkdir /scratch/e14/rmh561/croco/PAC12_75_cpl
cd /scratch/e14/rmh561/croco/PAC12_75_cpl
mkdir CROCO_Compile
mkdir rundir
mkdir /g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75_cpl
ln -s /g/data/e14/rmh561/croco/data/WRF_FILES/PAC12 /g/data/e14/rmh561/croco/data/WRF_FILES/PAC12_75_cpl
ln -s /g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75_cpl CROCO_FILES
ln -s /g/data/e14/rmh561/croco/data/WRF_FILES/PAC12_75_cpl WRF_FILES

Setup CROCO inputs:

Run loop_ln_croco_bry_cpl.bash to link in SODA boundary files from PAC12_75.

cd /g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75_cpl/
ln -s /g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75/croco_grd_PAC12.nc croco_grd.nc
cp /g/data/e14/rmh561/croco/archive/PAC12_75/PAC12_75_exp10/croco_rst_20141231.nc ./croco_rst_20131231.nc
ncap2 -A -s scrum_time=1073001600 croco_rst_20131231.nc croco_rst_20131231.nc
ln croco_rst_20131231.nc CROCO_rst_Y2014M01.nc

Change lots of paths in config files.

Fix WRF data links:
cd /g/data/e14/rmh561/croco/src/WRF/WRF/data/
rm *
cp --symbolic-link ../run/*
rm *.exe

*** Recompile

OASIS:
In /g/data/e14/rmh561/croco/src/oasis3-mct
cp -r build/* build_backup_pre19-04-2022/
cd util/make_dir
cp ~/croco/setup_gadi_env.sh ./
source setup_gadi_env.sh
make -f TopMakefileOasis3 BUILDROOT=$PWD

XIOS:
Already done with new setup_gadi_env.sh

CROCO:
cd CROCO_IN/
./jobcomp > jobcomp.log

WRF:
cd /g/data/e14/rmh561/croco/src/WRF/WRF/
Update paths in configure.wrf
./run_compile

*** Running procedure

./submitjob.sh

Ran into a problem where the time variable that the code wanted in the
CROCO restart was ocean_time, but the file only had scrum_time. This
is because the SCRIPTS_TOOLBOX is the old version of CROCO and this
hadn't been updated. I updated by hand.

Having fixed that I got it working with a few other minor changes.

Checked consistency with PAC12_75 - all good.

Updated/cleaned XIOS CROCO diagnostics - all good after replacing
radsw - swrad (one works with BULK_FLUXES, one without) and the
components of shflx (these are all calculated in WRF so not provided
individually).

** Smoothing SST
*** Initial setup (and IDL script for remapping):
I need to:
**** 1) Generate a remapping file rmp_ocnt_to_atmt_SMOOTH.nc file: 
***** The rmp file details
In the normal run, we use SCRIPR as an option in namcouple which
automatically generates the default rmp_ocnt_to_atmt_DISTWGT.nc file.

To understand this file:

num_links -> number of associated source and target grid point pairs
(for the standard run which uses 4 nearest neighbours distrance
weighted interpolation DISTWGT, this is a bit less than the
dst_grid_size=87676 * 4, with the bit less than arrising from the
masking).

src_address -> 1D array of size num_links with source address
dst_address -> 1D array of size num_links with destination address
remap_matrix -> num_links*num_wgts contains the weights associated
with each link. 

There are also a bunch of other variables associated with "corners"
and "rank" - but perhaps these are only used for conservative
interpolation? 

There is also the BILINEAR and BICUBIC options instead of DISTWGT -
but I don't think these can do smoothing (not sure)?

There is more information on these in the "SCRIPusers.pdf" of the
oasis3-mct/docs folder.

The scrip code has a test function the uses your input remapping file
to generate a source and destination array that is output as a netcdf
file - so this will be useful for testing.

***** Understanding Sebastien's IDL script: 

Actually it might not be too hard, I think I only need to modify the
"smooth atm values..." section to change the smoothing.

Input arguments:
- fileo -> croco_grd.nc containing (xi_rho, eta_rho, lon_rho, lat_rho,
           mask_rho). 
- filea -> A wrf grid file containing west_east and south_north IDs
  and XLONG_M or XLONG and XLAT_M or XLAT (where does this come from?)

- filecpla -> .nc file containing CPLMASK (which I can take from a WRF
  output file). 

- filesst -> netcdf file containing SST for testing.

sig -> Gaussian smoothing width in integer grid points.
cutratio -> Some rule about how large the weights have to be to keep
them. 
dmax -> Approximate distance in degrees between one atm point and its
4 closest ocean neighbours.

***** Setup and running Sebastien's IDL script:
mkdir /g/data/e14/rmh561/croco/data/OASIS_FILES/PAC12_75_cpl/
cd /g/data/e14/rmh561/croco/data/OASIS_FILES/PAC12_75_cpl/
ln -s /g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75/croco_grd_PAC12.nc croco_grd.nc
ln -s /g/data/e14/rmh561/croco/data/WRF_FILES/PAC12/wrfinput_d01_2014_01 wrf_grd.nc
ncrcat -v temp_surf -d time_counter,0,1 /g/data/e14/rmh561/croco/archive/PAC12_75_cpl/PAC12_75_cpl_exp02/20170101_20170131/croco_out_day.nc sst.nc

Modified paths in the script.

Downloaded Sebastien's scripts to /g/data/e14/rmh561/croco/src/SAXO
(has inquad, quadrilateral2square, etc. etc.):
svn checkout http://forge.ipsl.jussieu.fr/saxo/svn/trunk/SRC SAXO/SRC
Downloaded Sebastien's init.pro script and modified paths.

Base script are in ~/croco/PAC12_75_cpl/OASIS_IN/. TO run, use the pbs
submission script make_wgt.sub (see inside for details and IDL
commands).

Modified script to use a narrow Gaussian in y. This could probably be
done much better. 
**** 2) modify namcouple to use this file
TO do this, I just need to use a single transformation and the MAPPING
keyword. E.g. I think for SST the following would work:

Normal (to generate rmp_ocnt_to_atmt_DISTWGT.nc):
-------

CROCO_SST WRF_d01_EXT_d01_SST 1 3600 1 oce.nc EXPORTED
1599 503 522 168 ocnt atmt LAG=300
R  0  R  0
SCRIPR
DISTWGT LR SCALAR LATLON 1 4

Custom remapping using file rmp_ocnt_to_atmt_SMOOTHSST.nc:
--------------
CROCO_SST WRF_d01_EXT_d01_SST 1 3600 1 oce.nc EXPORTED
1599 503 522 168 ocnt atmt LAG=300
R  0  R  0
MAPPING
rcmp_ocnt_to_atmt_SMOOTH.nc dst

What do I use for this last? Probably dst (will be quicker/larger
tiles?). Also consider the optional additional option bfb, sum or
opt. sum or opt might be much faster than bfb (sacraficing some
round-off error).

Note: the masks.nc, grids.nc and areas.nc files are not needed when
using a custom remapping file.

**** 3) modify setup scripts to link in this remap file
I just added some code at the bottom of SCRIPTS_TOOLBOX/cpl_nam.sh to
link all rmp*.nc files in $CWORK/OASIS_FILES (which itself is linked
from scratch to gdata). 
**** RANDOM NOTES:
***** ACCESS-OM2
In access-om2, the namcouple contains minimal detailed because:

- CICE and MOM use the same grid and same time-step, so no remapping
  is needed (no transformations needed, so the 3rd last argument is 0
  and no options are given below). Note that o2i.nc and i2o.nc are the
  oasis restart files for the LAG and SEQ arguments.
- For atm -> cice we pre-calculate the remapping weights files and
  then use the "MAPPING" transformation as our only
  transformation. The options to this are the remapping weights file
  and . (Except for river runoff which doesn't need a remapping
  weights file, at least not in namcouple/oasis - no transformations
  used). 

NOTE: we're not using conservative remapping for heat flux atm->ocn.

*** Modifications for masking and python script (21/11/2022):

There were a bunch of errors in the python script that Lionel sent
me. This is a brief summary:
1. Longitude cyclicity on atmospheric grid.
2. Didn't deal with the case where the atmospheric point was right on
   top of an oceanic point properly in "outside of ocean grid
   weights".
3. nn/2 was giving a float (X.5) when nn was an integer which shifted
   things in the wrong direction in the smoothing. needed to use
   int(nn/2) [which was implicit in IDL].
4. Issues with the SST testing smooth at the bottom - needed to add
   expand_dims.
5. Some possible errors that Matthieu fixed in his version. I'm not
   sure if these were neccessary.

Note: There was no issue with C vs. F ordering.

I fixed these. I also changed it to smooth only in the x-direction
properly. I.e. with zero width in the y-direction (much more
efficient).

Finally, I introduced a mask for specifying the region over which
smoothing is applied. Specifically, this takes a mask of float value
between 0 and 1 on the atmospheric grid from a netcdf file and uses
this coefficient to apply the appropriate fraction of the smoothing
window or a window which does no smoothing (i.e. just a 1 at the
central point).

The script for making the smoothing mask is in Make_Smooth_Mask.ipynb.

The script for testing the .py script is in build_weights_test.ipynb.

The PBS submission script is in make_wgt_py.sub.

After much dealing with bugs, I've got this working. See the last
commit on:

https://github.com/rmholmes/TropPacCROCO-WRF/tree/remapping_mask_x_developmenthttps://github.com/rmholmes/TropPacCROCO-WRF/tree/remapping_mask_x_development

https://github.com/rmholmes/TropPacCROCO-WRF/commit/9f54fa07d7836e1eaeb0ffb81b75d35a8d8f8aa5https://github.com/rmholmes/TropPacCROCO-WRF/commit/9f54fa07d7836e1eaeb0ffb81b75d35a8d8f8aa5

** Ensembles

Ensembles created by initializing from different years of the
CROCO-only spinup.

First, I cleaned up the previous method by renaming the restart file
used for exp01 and exp03 in
/g/data/e14/rmh561/croco/data/CROCO_FILES/PAC12_75_cpl/ to
croco_rst_20131231_exp10end.nc. Then for the new ensemble I copied
/g/data/e14/rmh561/croco/archive/PAC12_75/PAC12_75_exp08/croco_rst_20141231.nc
to croco_rst_20131231_exp08end.nc, did the ncap2 replace scrum_time
thing and then linked croco_rst_Y2014M01.nc to that file instead.

This was setup and run as exp04 (out of a different control directory
cpl-2).

I've also setup 4 control run ensembles to do the same.

** PP81 coupled run
Easily setup using exp05. Note be careful with executables.
** Current feedback runs

exp06 setup to run with no current feedback. This simply involves
removing the lines in namcouple that pass UOCE/VOCE to the
atmosphere.

exp07 setup to run with no mesoscale current feedback. This simply
involves passing the smoothed UOCE/VOCE to the atmosphere. 

CPLMASK WRF modifications so that NoCFB is applied only over TIW
region: I need to basically input to versions of CPLMASK into the
cpl_rcv_sfcdrv routine in frames/module_cpl.F in the WRF code. This is
called from phys/module_surface_driver.F, which is called from
dyn_em/module_first_rk_step_part1.F, which contains input
CPLMASK=grid%cplmask.

There is also a bunch of mentions in the 'alloc' files in frame/ - is
this where they are read in? 

Also in the 'alloc.inc' files in inc/, but these are all automatically
generated from a Registry.

The Registry is in Registry/Registry.EM_COMMON

After doing a "./clean -a", I need to modify:
- cpl_rcv_sfcdrv in frames/module_cpl.F - DONE
- phys/module_surface_driver.F          - DONE
- dyn_em_module_first_rk_step_part1.F   - DONE
- Registry/Registry.EM_COMMON           - DONE

To recompile just did ./run_compile in this directory. This worked,
and I copied the executables into exes/coupled_cplmasku (use this for
a new NoCFB run).

Modifying the actual CPLMASK:
These are in the first (2014-01) wrfinput file as CPLMASK. So I just
need to add a CPLMASKU which has the masking I want. I
then use these executables for the NoCFB run, and the original
executables if not. I think I should probably do this once and check
if  makes a big difference.

Make_Smooth_Mask.ipynb modified to make the new input file. It's
really big (52GB) because of netcdf compression. But continuing
anyway...

Rerunning costs:
For full NoCFB:
1 member = 5*22 = 110kSU
5 members = 550kSU

For TIW output section of Control, NoTFB, NoMesoCFB and NoCFB:
4*4*22 = 352kSU

Total = 814kSU. So I have it, but not by that much.

Would be worth doing one member of NoCFB without TIWoutput to check
differences anyway.


** Analysis notes
*** First look at new masked-smoothing runs 7/12/2022:
- Yes, large mean state interhemispheric anomalies were an artifact of
the smoothing of the P-C upwelling - it's much smaller in the new run
(although similar sign).

- Mean state changes are subtle. Need to confirm statistical
significance before reading too much into them (hatch if the changes
are the same sign compared to every control ensemble?).

- NoCFB seems to have the largest/most coherent mean state changes
with a large-scale cooling and large zonal current and SSH changes.

- Significant changes in eddy wind work in all simulations - including
smooth-SST.

- Interesting changes in MKE and EKE - although more subtle.

- Some interesting looking changes in equatorial temperature variance
in the CFB experiments - if they're robust?

- Big changes in SSH variance and v variance away from equator in CFB
and NoMesoCFB.

- Differences between NoMesoCFB and NoCFB seem even larger in the
masked experiments - suggesting that the mean CFB effects are larger
in these new experiments (not sure if this is statistically significant)?

*** Heat budget analysis using Lisa's code

Using Lisa's Tracer_balance_code_LMaillard_v2;

need intel compilers loaded:
module load intel-compiler

Edit R_tools_fort.F to turn on the functions that you want (note: not
all of them compile successfully. E.g. rho_eos (but rho_eos1 works). I
had to activate #define DUKO_2001 in cppdefs.h to get make_buoy
working).

then do "make clean", "make".

In python3: 
import R_tools_fort as toolsF

get_tracer_evolution.F requires the following inputs:
Lm,Mm, N, pm, pn, 
u, v, z_r, z_w, dt, t, stflx, srflx, ghat, swr_frac, W, AkT.

Need to investigate:
swr_frac: need Hz.
ghat: stflx, srflx, hbl, swr_frac, alpha, beta.
Hz: probably fine
stflx: probably fine
srflx: probably fine
alpha, beta: alfabeta.F using rh0, t, s.
W: Yes this is omega. Can get it using get_omega from u and v if needed. 

So I think I have everything I need.

I won't capture sub-daily correlations between T and V, of course. Not
sure how important this is, but if I'm only trying to get the TIW eddy
contributions that's not really a problem?

Problem: I think the TS_VADV_SPLINES routine does an integral from the
bottom up. So I would need the full vertical column of output to do
everything properly. Rerun?

*** EKE budget:

From Holmes et al. 2016 I would need the friction and pressure
gradient operators to completely close the budget. I don't think I
really need to do this. 

Just look at BT and BC energy conversion sources:
u'u' dU/dx
u'v' dU/dy
u'w' dU/dz
v'v' dV/dy
u'v' dV/dx
u'w' dV/dz
w'b'
And the wind works which I've already calculated.

So need to calculate b using the EOS. Should be in Lisa's code, but
doesn't seem to be in the module at the moment. Need to look into that...

* PAC12_75_wrf - WRF only
Setup a WRF-only configuration initially just for scaling
calculations, but could be useful later on...
** To-do list
*** TODO Setup to use CROCO or CROCO-WRF SST as an input?
*** DONE Profiling with no output
    CLOSED: [2022-05-31 Tue 09:09]
*** TODO Test XIOS impact on profiling
** Procedure
In mynamelist.sh turn off USE_OCE and USE_XIOS_OCE.
Setup myjob.sh with required settings.
Change myenv_mypath.sh CONFIG!!

mkdir /scratch/e14/rmh561/croco/PAC12_75_wrf
cd /scratch/e14/rmh561/croco/PAC12_75_wrf
mkdir rundir
cd /g/data/e14/rmh561/croco/data/WRF_FILES
ln -s PAC12 PAC12_75_wrf
cd /scratch/e14/rmh561/croco/PAC12_75_wrf
ln -s /g/data/e14/rmh561/croco/data/WRF_FILES/PAC12_75_wrf WRF_FILES

Need to recompile WRF in uncoupled mode and then properly link in
these executables - see script changes.

Need to do "./clean -a" first...

Also need to compile XIOS without OASIS... But this does not work. So
now I'm trying without XIOS...

Without XIOS output is determined as follows: 
There are 2 ways to do this:

1) Go into the WRF/Registry/Registry.EM_COMMON file, and then look for
the variables you don't want. You can remove the ‘h’ from out I/O
column. But you need to recompile with a ./clean -a
2) Use the runtime io option, as described here
https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/v4.1/users_guide_chap5.html#runtimeio

For now I'm just setting the output frequency to be very low.

** Performance/scaling:
ATM time step = 30 seconds
ATM grid = 522 * 168
1051.2
| nx * ny  | TOT | TILE      | queue    | s/1000ts | hr/365d | cost (SU*hr/year) |
| 11 * 4   | 44  | 47.4 * 42 | express  | 426.0    | 124     | 5456              |
| 16 * 6   | 96  | 33 * 28   |          | 198.0    | 58      | 5568              |
| 24 * 6   | 144 | 22 * 28   |          | 140.0    | 41      | 5904              |
| 30 * 8   | 240 | 17 * 21   |          | 85.9     | 25      | 6000              |
| 32 * 12  | 384 | 16 * 14   |          | 53.3     | 15.6    | 5990              |
| 29 * 14  | 406 | 18 * 12   | "        | 50.9     | 14.9    | 6049              |
| 40 * 12  | 480 | 13 * 14   |          | 48.3     | 14.1    | 6768
| 48 * 13  | 624 | 11 * 13   |          | 41.0     | 12.0    | 7488
| 48 * 16  | 768 | 11 * 10.5 |          | 36.0     | 10.5    | 8064

| 18 * 12  | 216 | 29 * 14   | "        | 82.4     | 24.0    | 5184

522 factors: 2, 3, 6, 9, 18, 29, 58, 87, 174, 261
168 factors: 2, 3, 4, 6, 7, 8, 12, 14, 21, 24, 28, 42, 56, 84

Best configuration:
29*14=406 = 6690 CPU-hr/year (13380 on normal) at 15.5 hours/year

1:11:16 walltime with 432 CPUs (a full node) for February.

* Compression

Using nccompress I'm getting a factor of about 57% with compression.
* Analysis 
** to-do list (coupled):
*** DONE More comprehensive wind work and EKE/MKE analysis
    CLOSED: [2022-12-09 Fri 17:13]
*** TODO BT/BC conversion terms and EKE budget in general
*** TODO coupling coefficients (TC analysis)
*** TODO atmospheric analysis - how far do the anomalies extend?
*** TODO heat budget - to understand V'T' and disentangle TIWTV changes (direct or through enhanced stirring?)
*** TODO Temperature variance budgets (TIWTV)?
*** TODO Better understand SSH variance results - What about vertical structure?
*** TODO Some indication of vertical mixing changes in TFB/CFB runs.
*** DONE Statistical significance plots
    CLOSED: [2022-12-09 Fri 17:13]
*** DONE Plot net LW flux from coupled simulations?
    CLOSED: [2022-12-09 Fri 17:14]
*** TODO Compare different reanalyses: ORAS5, SODA and Glorys.
*** TODO Wavenumber spectra (take code from NumMix paper) - 
*** TODO T-lat space budgets?
** to-do list (mixing):
*** TODO Add TAO obs once longer time period.
*** TODO Quantify changes in TIW modulation
*** TODO Quantify rectification?
*** TODO Understand the vertical structure results
