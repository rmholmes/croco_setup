* General links and compilations
** CROCO documentation and website links:
https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.01.download.html
https://www.croco-ocean.org/
https://gitlab.inria.fr/croco-ocean -> development code
https://forum.croco-ocean.org/questions/

** WRF links:
CMS WRF for Gadi: https://github.com/coecms/WRF
CMS WRF wiki: http://climate-cms.wikis.unsw.edu.au/WRF - see bottom of
page for tutorials/instructions.

** OASIS links:
CMS OASIS wiki: http://climate-cms.wikis.unsw.edu.au/Using_Oasis
COSIMA OASIS?? https://github.com/COSIMA/oasis3-mct

* CROCO v1.1 tutorials and initial runs, with some WRF/OASIS
** CROCO-only run (climatological forcing, Benguela):
-------------------------------------------------

# Initial file edits:
- Edit the name in setup_new_config.sh and run it to create a clean
  directory.
- Edit cppdefs.h with required options (e.g. name, MPI, BULK_FLUX, etc.).
- Edit croco.in for time-step, length, input and output file names and
  all sort of other settings.
- Edit crocotools_param.m for input files setup (grid res, paths to
  src directories etc).
- Edit param.h for pre-compilation parameters (grid sizes, MPI sizes
  etc.)
- Edit start.m for paths.
- Edit submit_croco.sub for PBS submission parameters.

# Pre-processing:
start matlab (you can use ../matlab_load.sh)

start
make_grid:
        to make INPUT/croco_grd.nc
make_bulk:
        to make bulk forcing atmospheric forcing, could also do
        make_forcing for flux forcing, makes INPUT/croco_blk.nc)

make_bry:
        to make boundary forcing files. Can also do make_clim to have
        climatological nuding files that nudge into the interior. Note
        that for WOA13 based climatology boundary conditions you need
        to do make_forcing first in order to create wind stress inputs
        to use in the geostrophic (/Ekman?) calculation of velocity
        boundary conditions from T/S. Creates croco_bry.nc and
        croco_bry_Z.nc

make_ini:
        to make the initial conditions.

# Compilation:
Edit jobcomp if needed (shouldn't be needed).
To compile, do (creates croco executable):
./jobcomp > jobcomp.log

# To run:
qsub submit_croco.sub

output turns up in OUTPUT. The log is in OUTPUT/croco.out

** OASIS compilation with CROCO
I could not get this working with the CMS gadi oasis3-mct install
(e.g. module load openmpi/4.0.1, module use ~access/modules, module
load oasis3-mct-local/opmi.4.0.1,
/g/data/access/apps/oasis3-mct/ompi.4.0.1/). So in the end I had to
download the latest version of oasis3-mct and compile it myself. This
is located at /g/data/e14/rmh561/software/oasis3-mct/. To compile it I
got a version of the util/make_dir/make.inc from Scott Wales setup for
Gadi (see oasis3-mct_gadi_make.inc), put it in util/make_dir, did
source setup_gadi_env.sh (with intel-compiler/2019.5.281, netcdf/4.7.1
and openmpi/4.0.2 only, I'm pretty sure) and
make -f TopMakefileOasis3 BUILDROOT=$PWD

The compilation ends up in build/

Using this I could compile CROCO using the instructions at
(https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.16.coupling.simple.html). I.e.
# define OW_COUPLING
# define MRL_WCI
Then, edit all the paths in jobcomp and notably OASIS path
PRISM_ROOT_DIR
./jobcomp >& compile_coupled.log

I had a lot of trouble getting the toy model compiled...gave up.

** WRF-only tutorial:
# Compiling:
Cloned https://github.com/coecms/WRF
cd WRF/WRF
./run_compile

to compile WPS (required for real configs to prepare inputs/bcs etc.):
cd ../WPS/
./run_compile

# Running tutorial:
Follow WRF tutorial (basics tab
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/wrf.php),
all steps - but CMS has put together scripts to do
this. http://climate-cms.wikis.unsw.edu.au/How_to_run_WRF

# steps:
mkdir /scratch/e14/rmh561/wrf/test
cd /scratch/e14/rmh561/wrf/test/
WRF_ROOT=/g/data/e14/rmh561/software/WRF/
cp $WRF_ROOT/WPS/run_WPS.sh ./
<modify this script to use the right WRF_ROOT and add the storage
flag for gdata/e14>
mkdir geogrid
mkdir metgrid
cp $WRF_ROOT/WPS/geogrid/GEOGRID.TBL geogrid
cp $WRF_ROOT/WPS/namelist.wps ./
cp $WRF_ROOT/WPS/metgrid/METGRID.TBL.ARW metgrid/METGRID.TBL
qsub run_WPS.sh
# This should successfully create the input files by running ungrid.exe, metgrid.exe and geogrid.exe

cp $WRF_ROOT/WRF/run/* ./
<modify run_mpi and run_real to use the right WRF_ROOT, WRF rather
than WRFV3 and add the storage flag for gdata/e14>

qsub run_real
qsub run_mpi

Make edits in namelist.input as listed at
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/wrf.php

Successful!

For a restart run simple change the start date, end date and set
restart = .true. in namelist.input

For the SST run, follow the instructions
https://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/ungrib.php#SST,
with the appropriate changes in run_WPS.sh and namelist.wps. Run
metgrid again (this overwrites the previous inputs, adding just the
SST field). And then run the model again (qsub run_real, qsub
run_mpi).

** WRF-only ETP, ERA Interim (and updated compile)
git clean -xf in /g/data/e14/rmh561/software/WRF/ to recompile. 
cd WRF/
./run_compile (takes 40 mins).
cd ../WPS/
./run_compile (takes 7 mins).

To setup WPS:
mkdir /scratch/e14/rmh561/wrf/era_interim
git clone https://github.com/coecms/wps-era ./

modify namelist.wps for appropriate domain and time period. THe page
https://www2.mmm.ucar.edu/wrf/users/namelist_best_prac_wps.html#i_j_parent_start
is useful to list all the options.

make WPSDIR=/g/data/e14/rmh561/software/WRF/WPS

This should create all the required input files. Check them (it worked
first time!).

cp $WRF_ROOT/WRF/run/* ./

Modify namelist.input following
https://www2.mmm.ucar.edu/wrf/users/namelist_best_prac_wrf.html
Note: dx and dy must match the global attributes in the met_em*.nc
files. 

qsub run_real

Modify # of processors in run_mpi according to
https://forum.mmm.ucar.edu/phpBB3/viewtopic.php?t=5082,
by default, the decomposition is done automatically (see
README.namelist), but you can set it if you want in &domains section
of namelist.input. I tried with 32 but it said too many. Successful
with 16. Now going with 20.

qsub run_mpi

Successfully run a day. Now going to run a month (Nov 2005) with
reduced output a larger time step. I got a seg fault with a 180 second
time step. Reduced to 135 and it seems to work

The month run seemed to work. Although is the SST fixed at its initial
value? - LATER NOTE: I think this is because I didn't have
sst_update=1 in namelist.input

** Thoughts on implementation 10/10/2021
*** payu: Might work. Already deals with oasis. 

croco is quite simple with
minimal input and config files. However, Croco doesn't use the
standard namelist system in croco.in which contains all the input
timing (e.g. the actual dates within which the run starts and
finishes??). 

WRF is more complicated, but does use the standard namelist system.

Maybe ask CMS what they think about using payu to run croco and WRF?

** Next steps 10/10/2021:

Otherwise I have to start getting OASIS to work. However, this may not
be worth doing myself as the Toulouse guys likely already have scripts
to do the coupling, generate the weights and make the namcouple
file...

Also, still to look into:
- WRF with varying SSTs: -> probably just need to set sst_update in
  namelist.input
- WRF compile with OASIS: Follow instructions in croco tutorial - they
  have everything that is needed. 
- CROCO and WRF timing with restarts (can payu do this for me)?
- WRF-CROCO setup (note: I guess we pass the fluxes one way and the
  SST, usurf, vsurf the other?)

** Benguela coupled setup following CROCO tutorial:
*** Setup
mkdir /scratch/e14/rmh561/croco_wrf/Benguela_coupled
mkdir ~/croco/scratch/e14/rmh561/croco_wrf/Benguela_coupled
cd ~/croco/scratch/e14/rmh561/croco_wrf/Benguela_coupled
cp ~/croco/croco_tools_src/Coupling_tools/create_config ./
<edit for paths - many>
./create_config
<edit paths in run_env>
*** Compiling CROCO:
Copy jobcomp from ~/croco/Benguela_LR_cpl/
<edit paths -> note I had a problem with PRISM_ROOT, must point at
...../oasis3-mct/build>
./make_CROCO_compil
This was successful (I got croco.oa).
*** Compiling WRF:
This looks tricky. The CROCO tutorial have used a modified version of
3.7.1. The CMS version of WRF is 4.3, that I might need to modify?

**** Uncoupled WRF (as before)
Started off by re-compiling WRF as previously (this would be in
uncoupled mode). Modified ../build.env to use openmpi/4.0.2 rather
than 4.0.1. and then did "git clean -xf" in /g/data/e14/rmh561/WRF/,
cd WRF/, ./run_compile. This submits a job (takes ~40 mins), compiled
fine. I copied all the .exe files in main/, along with the
configure.wrf and the log (compile_job.o***) to 
/scratch/e14/rmh561/croco_wrf/wrf_exes/uncoupled_c2b02af69c856/

**** Coupled WRF
Again in WRF/WRF, first ./clean -a to clean the config (after copying
the exes!!). Then, cp configure.wrf.backup to configure.wrf and modify
configure.wrf to include the oasis flags following listed in step 6 of
the CROCO tutorial under "Compiling WRF". To use this I then commented
out the section in run_compile that overwrites the configure.wrf file
(I couldn't seem to get my own qsub script to work), and added it
under git. Saved this configure.wrf in
/scratch/e14/rmh561/croco_wrf/wrf_exes/coupled_3b5c894c918387/. Then
./run_compile. This worked, but only just squeaked in with 1:21 of the
1:30 walltime! .exe's copied to coupled_3b5.... 

By the way: the oasis coupling code is in frames/module_cpl_oasis3*. 

***** Notes:
Comparing the so generated WRF/configure.wrf to the version provided
from croco
(~/croco_wrf/Benguela_coupled/wrf_in/inputs.../configure.wrf.uncoupled)
seems to show most changes are architecture related (so I want mine!),
except perhaps a few new modules in WRF included in the "compile
without OMP or without high optimization")? So this is all
architecture related?

Comparing the croco uncoupled and coupled configure.wrf shows changes
only associated with OASIS. These are the ones listed in step 6 of the
CROCO tutorial under "Compiling WRF". These are made by hand in CROCO,
so I probably just need to figure out how to hack this in the CMS WRF
version? Should be pretty easy. Simply do the steps in run_compile all
by hand, edit the configure.wrf file with the required changes, and
then do the subsequent steps with this file. TODO...

***** NOW
I also found Guillaume's NOW setup, copied to the tar.gz in
/g/data/e14/rmh561/NOW. Looks like he has versions of
configure.wrf.coupled for use on raijin (most up to date in
wrf3.5.1_red_flx??). The Oasis inclusions there look very simple
(there's only a few of them, linked to OA3MCT_ROOT_DIR. Might be
easy?!?...

**** WPS
As above, cd /g/data/e14/rmh561/WRF/WPS
./run_compile

*** Pre-processing CROCO
in croco_in edited start.m and crocotools_param.m for paths (and
needed to add topo_smooth parameter). Then ran matlab and
start
make_grid
make_bulk
make_forcing
make_bry
make_ini

This all seemed to work fine, producing the required files in /scratch/.../croco_files/

*** Pre-processing WRF
** Indo-Pacific basin-wide CROCO only configuration 30/12/2021
*** Setup to first working run
Cloned ETPcroco setup.

Changed from FRC_BRY to CLIMATOLOGY in cppdefs.h

Updated other settings to reflect changes in grid size and
parallelization.

../matlab_load.sh
start
make_grid
make_forcing
make_bulk
make_ini
make_bry

Note: Failed first time because I had the minimum longitude in degrees
east rather than negative degrees west. I guess I have to use the same
for each of the two limits. 

Note 2: Set the res and limits in crocotools_param, and then use the
printed output of make_grid to set LLm and MMm in param.h

For IPBW I picked:
lonmin =  -330;   % Minimum longitude [degree east]
lonmax =  -70;   % Maximum longitude [degree east]
latmin = -34;   % Minimum latitudeF  [degree north]
latmax = 60;   % Maximum latitude  [degree north]
dl = 1/4;

I got LLm=1039 and MMm=446

So I split my grid into 24 * 10 

Once output produced do compilation:
./jobcomp > jobcomp.log

Something wrong with netcdf libraries; for some reason the include
path wasn't working, didn't include netcdf.inc. So I hard coded
NETCDFLIB and NETCDFINC. NETCDFINC with nf-config --includedir doesn't
seem to pick up the /apps/netcdf/4.7.1/include/Intel directory (just
getting include).

Got it running but it crashed immediately with a blow-up
error. Dropping the time step by a factor of 3 to 1200s seems to have
worked. This was working with FRC_BRY and not CLIMATOLOGY.

Now trying to get a run with CLIMATOLOGY and a closed northern
boundary working. 

Note: run_start_date, run_end_date and output_time_steps are only used
when USE_CALENDAR is active, in which case DT_AVG etc. overwrite NAVG
etc.

Had to do "make_clim" in matlab to get croco_clm.nc; this doesn't seem
to work due to a netcdf close problem that I can probably fix... But
for now reverting to FRC_BRY (but keeping a closed northern
boundary).

I/O is a massive performance bottleneck - it is taking ages. Too much
output + a big domain + serial I/O?

*** Parallel I/O
There are a few utilities available to setup a parallel I/O run, which
should make things run much quicker.

Activate PARALLEL_FILES in cppdefs.h and recompile

To get this setup, first cd to Compile, source set_gadi_env.sh and do
gmake partit
gmake ncjoin

copy the partit and ncjoin executables to the appropriate
locations. Then in INPUT do:
./partit 24 10 croco_grd.nc
./partit 24 10 croco_bry.nc
./partit 24 10 croco_bry_Z.nc
./partit 24 10 croco_blk.nc
./partit 24 10 croco_frc.nc
./partit 24 10 croco_ini.nc

This creates 240 files for each type above. Then run as normal.

To rejoin at the end in OUTPUT do:
ncjoin croco_avg.*.nc

Takes a while (e.g. ~10 secs per time record for croco_avg.nc, or 5
mins for 30 days). 

This is wayyyyyyy quicker. 

Previous run speed: 20-25 days / hour on 240 CPUs, 
New run speed: 600 days / hour on 240 CPUs!!!
Equivalent to 40 years / day. That's almost as fast as ACCESS-OM2-1.


*** Spinup run
Started a single year spinup run with 5-daily output.

This ran through 3/4 of the year in 25 mins or so, and then crashed
with a blow-up. 

Combining the output does take a while. Can I run this with mpirun?

** Pacific basin-wide CROCO
As for Indo-Pacific with western boundary at -260E. 

Mask is ridiculously easy to edit. Did this to remove Atlantic and fix
bays etc. while doing make_grid
* CROCO v1.2 - January 2022
Running quickly through the updated tutorial. A few initial notes:

- They now have their own version of WRF (which includes wave coupling
  and some other things) - https://github.com/wrf-croco/WRF, this is
  version 4.2.1, and I need the same version of WPS (from
  https://github.com/wrf-model/WPS.git, with git checkout tags/v4.2)
  to get it working.

- I could use create_config.bash to create the configs directory - but
  this gives me lots of extra files that I don't think I need. So lets
  just stick with what I have.
** Quick re-run through Benguela_LR for checks on compile etc.

- updated jobcomp (and setup_raw_config) in base to v1.2.
- ran setup_raw_config.
- modified paths in start.m and crocotools_param.m
- Matlab; start, make_grid, make_bulk, make_forcing, make_bry,
  make_ini
- Modify param.h (MPI settings) and cppdefs.h (MPI, FRC_BRY,
  BULK_FLUX). 
- Compilation; ran into a problem with netcdf -> needed to hard code
  NETCDFLIB and NETCDFINC In jobcomp as above for Indo-Pacific run.
- Modified output paths in croco.in
- Runs fine.
*** Interannual forcing [FAILED]
- A few modifications as for tutorial
(https://croco-ocean.gitlabpages.inria.fr/croco_doc/tutos/tutos.05.prepro.matlab.inter.html)
- Matlab; Fails on the data download step of make_CFSR (seems as
  before). make_OGCM (with data download) seems to work. I think I
  need to put some work into this (see readme files in the various
  crocotools directories).
** Coupling - toy [FAILED]
*** OASIS compilation:
Checked out latest OASIS3-mct version (on OASIS3-MCT_5.0 branch in
/g/data/e14/rmh561/oasis3-mct/).
Copied in my make.inc from croco configs directory.
source ~/croco/setup_gadi_env.sh
make realclean -f TopMakefileOasis3 > oasis_clean.out
make -f TopMakefileOasis3 BUILDROOT=$PWD > oasis_make.out
# Note: The buildroot was essential otherwise i got a permissions error.
Seems to have worked.
*** CROCO compilation:
Activate OW_Coupling in cppdefs.h
./jobcomp >& compile_coupled.log
Successfull.
*** TOY model compilation:
cp -r croco_src/SCRIPTS/SCRIPTS_COUPLING/TOY_IN ./.
cd TOY_IN
Modify Makefile for Gadi (took a while to get this working, see copy
in this directory).
Compiled fine. 
But the toy model requires input .nc files, which I don't have... They
can be created from model output using the create_oasis_toy_files.sh
script in the SCRIPTS_COUPLING - but I don't have a ww3 run to do this
from.

I could do this from WRF -> but need the right output (e.g. TAUX is
not output from my current ETP run).

** Coupling - CROCO-WRF, following tutorial
Note: I started doing this with BENGUELA_LR, but realised it's
probably easier to just do an ETP config, since I already have that
running for an ETP domain. So after step 6, prior to WPS
pre-processing, I redid it for ETP_cpl. See notes for "ETP_cpl" below
for modifications. 

*** 1 Architecture setup:
cp croco_src/create_config.bash ./
edit create_config.bash for BENGUELA_cpl with all-prod-cpl.
Note; a backup of this file is inside the BENGUELA_cpl folder as
.bck. 
git init, add *
Edit myenv_mypath.sh for source setup_gadi_env.sh, paths, compilers
etc.
source myenv_mypath.sh
*** 2 OASIS compilation
As above in "coupling toy"
*** 3 CROCO pre-processing:
edit crocotools_param.m in PREPRO/croco directory
matlab:
   start 
   make_grid
   make_forcing
   make_bry
   make_ini
ls /scratch/e14/rmh561/croco/BENGUELA_cpl/CROCO_FILES/
croco_bry.nc  croco_bry_Z.nc  croco_frc.nc  croco_grd.nc  croco_ini.nc
**** ETP_cpl
Modified crocotools_param.m for a few names (didn't need to do lonmin,
latmin etc.), and modified
make_grid_from_WRF.m to use the ETP_wrf run done before. Then
matlab:
   start
   make_grid_from_WRF
   make_forcing
   make_bry
   make_ini
Take LLm and MMm from make_grid printed output and use in param.h.
Note: I messed up the BENGUELA croco input - would need to
regenerate. 
*** 4 CROCO compilation
Modify jobcomp, cppdefs.h and param.h according to Benguela_LR in CROCO_IN/
Note that the jobcomp Compile directory is just local, not in rundir
link, now. 
./jobcomp &> jobcomp_coupled.log
*** 5 WRF compilation
First try with CLEX CMS based WRF from /g/data/e14/rmh561/WRF/

The below is copied from above (including executables, which I'd
copied previously to /scratch/e14/rmh561/croco_wrf/wrf_exes/) from my
earlier compilation:
**** Uncoupled WRF (as before)
Started off by re-compiling WRF as previously (this would be in
uncoupled mode). Modified ../build.env to use openmpi/4.0.2 rather
than 4.0.1. and then did "git clean -xf" in /g/data/e14/rmh561/WRF/,
cd WRF/, ./run_compile. This submits a job (takes ~40 mins), compiled
fine. I copied all the .exe files in main/, along with the
configure.wrf and the log (compile_job.o***) to 
/scratch/e14/rmh561/croco_wrf/wrf_exes/uncoupled_c2b02af69c856/

**** Coupled WRF
Again in WRF/WRF, first ./clean -a to clean the config (after copying
the exes!!). Then, cp configure.wrf.backup to configure.wrf and modify
configure.wrf to include the oasis flags following listed in step 6 of
the CROCO tutorial under "Compiling WRF". To use this I then commented
out the section in run_compile that overwrites the configure.wrf file
(I couldn't seem to get my own qsub script to work), and added it
under git. Saved this configure.wrf in
/scratch/e14/rmh561/croco_wrf/wrf_exes/coupled_3b5c894c918387/. Then
./run_compile. This worked, but only just squeaked in with 1:21 of the
1:30 walltime! .exe's copied to coupled_3b5.... 

By the way: the oasis coupling code is in frames/module_cpl_oasis3*. 
    
*** 6 WPS compilation
As above, WPS is already compiled with executables in the WPS root
directory.

*** WPS pre-processing
For a first attempt I'm going to use the COE's ERA-Interim setup. This
was cloned from /scratch/e14/rmh561/wrf/ETP_WRF to
/scratch/e14/rmh561/wrf/ETP_WRF_clean, I adjusted the time in
namelist.wps to 2005-01 -> 2005-03 and then did make
WPSDIR=/g/data/e14/rmh561/WRF/WPS to make the files. Then copied
met_em.d01* and geo_em.d01.nc to
/scratch/e14/rmh561/croco/ETP_cpl/WRF_FILES/

*** WRF/real pre-processing
In WRF_IN, modified run_real and configure.namelist.real for paths and
to copy values in the ERA-Interim wrf (although there are some
differences). 

Note: Don't set nprocX and nprocY - leave them as -1 and it'll do it
automatically.

I couldn't get this working, so instead I just copied the already
created wrfbdy_d01 and wrfinput_d01 from ETP_WRF run previously. 

Also copied the namelist.input file from this run, and modified:
in &physics: sst_update = 1 if your are coupling with an ocean model
in &domains: num_ext_model_couple_dom = X : number of domains of the
other model.

Also made CPLMASK=1 everywhere in wrfinput_d01.

Also modified create_oasis_grids_from_wrf.sh and ran (including a
chmod +x on this file and the to_wrf_stag_grid.sh) to create
masks.wrf.nc and grids.wrf.nc ins WRF_FILES.

The CROCO tutorial for coupling then proceeds as if you were using the
toy model... It's not very well organised...

So instead I just continued trying to do WRF-CROCO.
*** OASIS setup
This is all done automatically by submit_job.sh. I tried to get this
working by modifying a number of scripts for setup etc. The OASIS
stuff seemed to work ok. However, I ended up giving up running into
qsub submission issues (e.g. app.conf wasn't seeming to be
accepted). I could probably solve these, but it's not obvious...

* CURRENT STATUS:
** CROCO only - got to figure out interannual forcing
** CROCO-TOY -> would probably run if I could generate the toy input files from WRF
** CROCO-WRF with COE WRF ERA-Interim -> ran into submitjob problems qsub
** CROCO-WRF with CROCO WRF -> haven't tried yet.
